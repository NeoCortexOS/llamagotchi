{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "from langchain.retrievers import TimeWeightedVectorStoreRetriever\n",
    "from langchain.schema import BaseLanguageModel, Document\n",
    "from langchain.vectorstores import FAISS\n",
    "import numpy as np\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bin = \"/media/captdishwasher/Samshmung/horenbergerb/llama/llama.cpp/models/llama/converted/llama_13b_ggml_q4_3.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /media/captdishwasher/Samshmung/horenbergerb/llama/llama.cpp/models/llama/converted/llama_13b_ggml_q4_3.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 6 (mostly Q4_3)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =  73.73 KB\n",
      "llama_model_load_internal: mem required  = 11359.03 MB (+ 3216.00 MB per state)\n",
      "llama_init_from_file: kv self size  = 3200.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "def normalize(v):\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm == 0: \n",
    "       return v\n",
    "    return v / norm\n",
    "\n",
    "embeddings_model = LlamaCppEmbeddings(model_path=model_bin, n_ctx=2048, n_threads=6, n_batch=512)\n",
    "def embedding_function(query):\n",
    "    x = embeddings_model.embed_query(query)\n",
    "    return normalize(x)\n",
    "\n",
    "def relevance_score_fn(score: float) -> float:\n",
    "    \"\"\"Return a similarity score on a scale [0, 1].\"\"\"\n",
    "    # This is probably a really dumb similarity score\n",
    "    # The embeddings don't seem to have any bounds on what the vector magnitudes can be\n",
    "    # So I normalized them to be between 0 and 1\n",
    "    # Then FAISS takes the squared Euclidean distance between these vectors\n",
    "    # So the output has a max value of 4 and a minimum of 0\n",
    "    # But it needs to be between 0 and 1\n",
    "    # Hence the function:\n",
    "    return (4.0 - score) / 4\n",
    "\n",
    "\n",
    "def create_new_memory_retriever():\n",
    "    \"\"\"Create a new vector store retriever unique to the agent.\"\"\"\n",
    "    embedding_size = 5120\n",
    "    index = faiss.IndexFlatL2(embedding_size)\n",
    "    vectorstore = FAISS(embedding_function, index, InMemoryDocstore({}), {}, relevance_score_fn=relevance_score_fn)\n",
    "    return TimeWeightedVectorStoreRetriever(vectorstore=vectorstore, other_score_keys=[\"importance\"], k=15)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3236.76 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  3104.24 ms /     8 tokens (  388.03 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  3105.98 ms\n",
      "\n",
      "llama_print_timings:        load time =  3236.76 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2970.63 ms /     7 tokens (  424.38 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2972.03 ms\n",
      "\n",
      "llama_print_timings:        load time =  3236.76 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2541.01 ms /     3 tokens (  847.00 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2542.72 ms\n",
      "\n",
      "llama_print_timings:        load time =  3236.76 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2778.30 ms /     6 tokens (  463.05 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2780.61 ms\n",
      "\n",
      "llama_print_timings:        load time =  3236.76 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2576.15 ms /     3 tokens (  858.72 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2578.31 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['0843d5e8-c1f1-4450-ab1c-b9f8e960cf22',\n",
       " 'a60630e3-cb21-4519-8799-856c252de0fb',\n",
       " 'f51935a2-1487-40ad-98c4-e9495444ca5e',\n",
       " '2abdaf55-193d-42e0-8996-98b2ff34f0fd',\n",
       " '0ed19bf7-6281-44d4-8519-7d770d59a626']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_retriever = create_new_memory_retriever()\n",
    "\n",
    "memories = [\"peepee poopoo\", \"stinky doodoo\", \"flowers\", \"filthy and putrid\", \"Bob Ross\"]\n",
    "documents = [Document(page_content=memory) for memory in memories]\n",
    "\n",
    "memory_retriever.add_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3236.76 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2819.97 ms /     6 tokens (  470.00 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2821.72 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{3: (Document(page_content='filthy and putrid', metadata={'last_accessed_at': datetime.datetime(2023, 4, 29, 16, 32, 38, 358177), 'created_at': datetime.datetime(2023, 4, 29, 16, 32, 38, 358177), 'buffer_idx': 3}),\n",
       "  0.9033975228667259),\n",
       " 1: (Document(page_content='stinky doodoo', metadata={'last_accessed_at': datetime.datetime(2023, 4, 29, 16, 32, 38, 358177), 'created_at': datetime.datetime(2023, 4, 29, 16, 32, 38, 358177), 'buffer_idx': 1}),\n",
       "  0.8958556354045868),\n",
       " 2: (Document(page_content='flowers', metadata={'last_accessed_at': datetime.datetime(2023, 4, 29, 16, 32, 38, 358177), 'created_at': datetime.datetime(2023, 4, 29, 16, 32, 38, 358177), 'buffer_idx': 2}),\n",
       "  0.8700922429561615),\n",
       " 0: (Document(page_content='peepee poopoo', metadata={'last_accessed_at': datetime.datetime(2023, 4, 29, 16, 32, 38, 358177), 'created_at': datetime.datetime(2023, 4, 29, 16, 32, 38, 358177), 'buffer_idx': 0}),\n",
       "  0.8667032569646835),\n",
       " 4: (Document(page_content='Bob Ross', metadata={'last_accessed_at': datetime.datetime(2023, 4, 29, 16, 32, 38, 358177), 'created_at': datetime.datetime(2023, 4, 29, 16, 32, 38, 358177), 'buffer_idx': 4}),\n",
       "  0.6548972129821777)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_retriever.get_salient_docs(\"yucky, nasty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3236.76 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  3107.75 ms /     8 tokens (  388.47 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  3109.13 ms\n",
      "\n",
      "llama_print_timings:        load time =  3236.76 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2948.27 ms /     7 tokens (  421.18 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2949.61 ms\n",
      "\n",
      "llama_print_timings:        load time =  3236.76 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2635.03 ms /     3 tokens (  878.34 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2636.60 ms\n",
      "\n",
      "llama_print_timings:        load time =  3236.76 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2758.34 ms /     6 tokens (  459.72 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2759.89 ms\n",
      "\n",
      "llama_print_timings:        load time =  3236.76 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2625.01 ms /     3 tokens (  875.00 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2626.59 ms\n",
      "\n",
      "llama_print_timings:        load time =  3236.76 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2794.51 ms /     6 tokens (  465.75 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2795.86 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='filthy and putrid', metadata={}), 0.7402871549129486),\n",
       " (Document(page_content='stinky doodoo', metadata={}), 0.7228175699710846),\n",
       " (Document(page_content='peepee poopoo', metadata={}), 0.7214316427707672),\n",
       " (Document(page_content='Bob Ross', metadata={}), 0.6299707293510437)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = 5120\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "vectorstore = FAISS(embedding_function, index, InMemoryDocstore({}), {}, relevance_score_fn=relevance_score_fn)\n",
    "\n",
    "vectorstore.add_documents(documents)\n",
    "vectorstore.similarity_search_with_relevance_scores('stinky yucky')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  2283.82 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2186.93 ms /     8 tokens (  273.37 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2189.45 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0000000e+00 2.2389171e-01 3.4028235e+38 3.4028235e+38]]\n",
      "[[ 0  1 -1 -1]]\n",
      "0.22389188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  2283.82 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2043.72 ms /     7 tokens (  291.96 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2045.95 ms\n"
     ]
    }
   ],
   "source": [
    "embedding1 = np.array([embedding_function(memory_content1)], dtype=np.float32)\n",
    "embedding2 = np.array([embedding_function(memory_content2)], dtype=np.float32)\n",
    "scores, indices = index.search(embedding1, k=4)\n",
    "print(scores)\n",
    "print(indices)\n",
    "\n",
    "print(np.square(np.linalg.norm(embedding1 - embedding2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
