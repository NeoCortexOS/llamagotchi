{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import ctypes\n",
    "import torch\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /media/captdishwasher/Samshmung/horenbergerb/llama/llama.cpp/models/vicuna-13B-1.1-GPTQ-4bit-128g-GGML/vicuna-13B-1.1-GPTQ-4bit-32g.GGML.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 4 (mostly Q4_1, some F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 9934493.73 KB\n",
      "llama_model_load_internal: mem required  = 11749.65 MB (+ 1608.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  = 1600.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "model_bin = \"/media/captdishwasher/Samshmung/horenbergerb/llama/llama.cpp/models/vicuna-13B-1.1-GPTQ-4bit-128g-GGML/vicuna-13B-1.1-GPTQ-4bit-32g.GGML.bin\"\n",
    "\n",
    "LLM = Llama(model_path=model_bin, n_ctx=2048, n_threads=7, use_mmap=False, n_batch=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits_and_probs(prompt):\n",
    "    tokens = LLM.tokenize(prompt.encode('utf-8'))\n",
    "    LLM.reset()\n",
    "    LLM.eval(tokens)\n",
    "    logits = LLM._logits()\n",
    "    logits = np.array(logits[0])\n",
    "    probs=F.softmax(torch.from_numpy(logits)).numpy()\n",
    "    return logits, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logits(logits, bins=50):\n",
    "    counts, bins = np.histogram(logits, bins=bins)\n",
    "    plt.stairs(counts, bins)\n",
    "    plt.title('Histogram of logits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probs(probs, bins=1000, bins_to_skip=2):\n",
    "    counts, bins = np.histogram(probs, bins=bins)\n",
    "    # Observing that the vast majority of tokens are in the first bin, i.e. have incredibly small probability.\n",
    "    print('There are {} tokens outside the {} smallest bins.'.format(len(np.where(probs>bins[bins_to_skip])[0]), bins_to_skip))\n",
    "    # Don't plot the smallest bins since they are not interesting and dwarf all other bins\n",
    "    plt.stairs(counts[bins_to_skip:], bins[bins_to_skip:])\n",
    "    plt.title('Histogram of Token Probabilities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_most_likely_tokens(probs, num_tokens_to_plot=10):\n",
    "    ind = np.argpartition(probs, -num_tokens_to_plot)[-num_tokens_to_plot:]\n",
    "\n",
    "    likely_tokens = []\n",
    "    token_probs = []\n",
    "\n",
    "    for token in ind:\n",
    "        c_token = [(ctypes.c_int)(*[token])]\n",
    "        print('Token: {}, Prob: {}'.format(LLM.detokenize(c_token), probs[token]))\n",
    "        # print('Token: {} Detokenized: {}'.format(token, LLM.detokenize(c_token)))\n",
    "        likely_tokens.append(LLM.detokenize(c_token))\n",
    "        token_probs.append(probs[token])\n",
    "\n",
    "    plt.bar(likely_tokens, token_probs)\n",
    "    plt.title('Bar chart of 10 most likely tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUMAN: You a creative scifi worldbuilding assistant. You are tasked with randomly generating a galaxy. First generate an astronomical description of the galaxy, noting its morphology and a few distinguishing characteristics. Then output the name of the galaxy. Format your answer as JSON with the following fields: [\"galaxy_description\", \"galaxy_name\"].\n",
      "\n",
      "ASSISTANT:\n",
      " {\n",
      "\"galaxy\\_description\": \"The galaxy is a barred spiral galaxy with a prominent central bulge and two major arms. It has a diameter of approximately 100,000 light-years and contains over 400 billion stars. The galaxy is surrounded by a dense halo of dark matter and a thin disk of gas and dust. The disk is divided into four main regions: the bulge, the spiral arms, the inter-arm region, and the halo. The galaxy is home to a variety of celestial objects, including stars, planets, nebulas, black holes, and satellite galaxies.\",\n",
      "\"galaxy\\_name\": \"The Milky Way\"\n",
      "} package com.example.coolweather.app;\n",
      "\n",
      "import android.RssDataSource;\n",
      "import android.annotation.SuppressLint;\n",
      "import android.content.Context;\n",
      "import"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"HUMAN: You a creative scifi worldbuilding assistant. You are tasked with randomly generating a galaxy. First generate an astronomical description of the galaxy, noting its morphology and a few distinguishing characteristics. Then output the name of the galaxy. Format your answer as JSON with the following fields: [\"galaxy_description\", \"galaxy_name\"].\n",
    "\n",
    "ASSISTANT:\"\"\"\n",
    "print(prompt)\n",
    "\n",
    "tokens = LLM.tokenize(prompt.encode('utf-8'))\n",
    "count = 0\n",
    "for token in LLM.generate(tokens, top_k=40, top_p=0.8, temp=1.2, repeat_penalty=1.1):\n",
    "    print(LLM.detokenize([token]).decode(\"utf-8\") , end='')\n",
    "    count += 1\n",
    "    if count >= 200:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUMAN: You a creative scifi worldbuilding assistant.\n",
      "You are tasked with randomly generating a galaxy. Your output should be in a JSON format. The fields of the JSON should be: [\"galaxy_name\", \"galaxy_morphology\", \"description\", \"notable_features\"].  Fill in each of the fields creatively, using scientific and astronomical terms. Be creative. Make the galaxy unique and unusual.\n",
      "\n",
      "ASSISTANT:{\n",
      "\n",
      "\"galaxy\\_name\": \"Nebulae Ophiuchi\",\n",
      "\"galaxy\\_morphology\": \"Barred Spiral Galaxy\",\n",
      "\"description\": \"Nebulae O"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/media/captdishwasher/Samshmung/horenbergerb/llama/llamagotchi/star_trek/astronomy.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/llamagotchi/star_trek/astronomy.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m tokens \u001b[39m=\u001b[39m LLM\u001b[39m.\u001b[39mtokenize(prompt\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/llamagotchi/star_trek/astronomy.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/llamagotchi/star_trek/astronomy.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m LLM\u001b[39m.\u001b[39mgenerate(tokens, top_k\u001b[39m=\u001b[39m\u001b[39m40\u001b[39m, top_p\u001b[39m=\u001b[39m\u001b[39m0.95\u001b[39m, temp\u001b[39m=\u001b[39m\u001b[39m1.2\u001b[39m, repeat_penalty\u001b[39m=\u001b[39m\u001b[39m1.1\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/llamagotchi/star_trek/astronomy.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mprint\u001b[39m(LLM\u001b[39m.\u001b[39mdetokenize([token])\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m) , end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/llamagotchi/star_trek/astronomy.ipynb#W6sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/media/captdishwasher/Samshmung/horenbergerb/llama/llama-cpp-python/llama_cpp/llama.py:307\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, temp, repeat_penalty, reset)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset()\n\u001b[1;32m    306\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 307\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval(tokens)\n\u001b[1;32m    308\u001b[0m     token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample(\n\u001b[1;32m    309\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m    310\u001b[0m         top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[1;32m    311\u001b[0m         temp\u001b[39m=\u001b[39mtemp,\n\u001b[1;32m    312\u001b[0m         repeat_penalty\u001b[39m=\u001b[39mrepeat_penalty,\n\u001b[1;32m    313\u001b[0m     )\n\u001b[1;32m    314\u001b[0m     tokens_or_none \u001b[39m=\u001b[39m \u001b[39myield\u001b[39;00m token\n",
      "File \u001b[0;32m/media/captdishwasher/Samshmung/horenbergerb/llama/llama-cpp-python/llama_cpp/llama.py:205\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_past \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(n_ctx \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(batch), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokens_consumed)\n\u001b[1;32m    204\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_tokens \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(batch)\n\u001b[0;32m--> 205\u001b[0m return_code \u001b[39m=\u001b[39m llama_cpp\u001b[39m.\u001b[39;49mllama_eval(\n\u001b[1;32m    206\u001b[0m     ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx,\n\u001b[1;32m    207\u001b[0m     tokens\u001b[39m=\u001b[39;49m(llama_cpp\u001b[39m.\u001b[39;49mllama_token \u001b[39m*\u001b[39;49m \u001b[39mlen\u001b[39;49m(batch))(\u001b[39m*\u001b[39;49mbatch),\n\u001b[1;32m    208\u001b[0m     n_tokens\u001b[39m=\u001b[39;49mllama_cpp\u001b[39m.\u001b[39;49mc_int(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_tokens),\n\u001b[1;32m    209\u001b[0m     n_past\u001b[39m=\u001b[39;49mllama_cpp\u001b[39m.\u001b[39;49mc_int(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_past),\n\u001b[1;32m    210\u001b[0m     n_threads\u001b[39m=\u001b[39;49mllama_cpp\u001b[39m.\u001b[39;49mc_int(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_threads),\n\u001b[1;32m    211\u001b[0m )\n\u001b[1;32m    212\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mint\u001b[39m(return_code) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    213\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mllama_eval returned \u001b[39m\u001b[39m{\u001b[39;00mreturn_code\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/media/captdishwasher/Samshmung/horenbergerb/llama/llama-cpp-python/llama_cpp/llama_cpp.py:284\u001b[0m, in \u001b[0;36mllama_eval\u001b[0;34m(ctx, tokens, n_tokens, n_past, n_threads)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mllama_eval\u001b[39m(\n\u001b[1;32m    278\u001b[0m     ctx: llama_context_p,\n\u001b[1;32m    279\u001b[0m     tokens,  \u001b[39m# type: Array[llama_token]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    282\u001b[0m     n_threads: c_int,\n\u001b[1;32m    283\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m c_int:\n\u001b[0;32m--> 284\u001b[0m     \u001b[39mreturn\u001b[39;00m _lib\u001b[39m.\u001b[39;49mllama_eval(ctx, tokens, n_tokens, n_past, n_threads)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"HUMAN: You a creative scifi worldbuilding assistant.\n",
    "You are tasked with randomly generating a galaxy. Your output should be in a JSON format. The fields of the JSON should be: [\"galaxy_name\", \"galaxy_morphology\", \"description\", \"notable_features\"].  Fill in each of the fields creatively, using scientific and astronomical terms. Be creative. Make the galaxy unique and unusual.\n",
    "\n",
    "ASSISTANT:{\"\"\"\n",
    "print(prompt)\n",
    "\n",
    "tokens = LLM.tokenize(prompt.encode('utf-8'))\n",
    "count = 0\n",
    "for token in LLM.generate(tokens, top_k=40, top_p=0.95, temp=1.2, repeat_penalty=1.1):\n",
    "    print(LLM.detokenize([token]).decode(\"utf-8\") , end='')\n",
    "    count += 1\n",
    "    if count >= 100:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
