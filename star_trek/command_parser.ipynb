{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import ctypes\n",
    "import torch\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /media/captdishwasher/Samshmung/horenbergerb/llama/llama.cpp/models/llama/converted/llama_13b_ggml_q4_3.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 6 (mostly Q4_3)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 9534493.73 KB\n",
      "llama_model_load_internal: mem required  = 11359.03 MB (+ 1608.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  = 1600.00 MB\n",
      "llama_apply_lora_from_file_internal: applying lora adapter from '/media/captdishwasher/Samshmung/horenbergerb/llama/llama.cpp/models/SuperCOT-LoRA/13b/ggml/cutoff-2048/ggml-adapter-model.bin' - please wait ...\n",
      "llama_apply_lora_from_file_internal: r = 8, alpha = 16, scaling = 2.00\n",
      "llama_apply_lora_from_file_internal: warning: using a lora adapter with a quantized model may result in poor quality, use a f16 or f32 base model with --lora-base\n",
      ".................... done (21578.80 ms)\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "model_bin = \"/media/captdishwasher/Samshmung/horenbergerb/llama/llama.cpp/models/llama/converted/llama_13b_ggml_q4_3.bin\"\n",
    "lora_bin = \"/media/captdishwasher/Samshmung/horenbergerb/llama/llama.cpp/models/SuperCOT-LoRA/13b/ggml/cutoff-2048/ggml-adapter-model.bin\"\n",
    "\n",
    "LLM = Llama(model_path=model_bin, lora_path=lora_bin, n_ctx=2048, n_threads=7, use_mmap=False, n_batch=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits_and_probs(prompt):\n",
    "    tokens = LLM.tokenize(prompt.encode('utf-8'))\n",
    "    LLM.reset()\n",
    "    LLM.eval(tokens)\n",
    "    logits = LLM._logits()\n",
    "    logits = np.array(logits[0])\n",
    "    probs=F.softmax(torch.from_numpy(logits)).numpy()\n",
    "    return logits, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logits(logits, bins=50):\n",
    "    counts, bins = np.histogram(logits, bins=bins)\n",
    "    plt.stairs(counts, bins)\n",
    "    plt.title('Histogram of logits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probs(probs, bins=1000, bins_to_skip=2):\n",
    "    counts, bins = np.histogram(probs, bins=bins)\n",
    "    # Observing that the vast majority of tokens are in the first bin, i.e. have incredibly small probability.\n",
    "    print('There are {} tokens outside the {} smallest bins.'.format(len(np.where(probs>bins[bins_to_skip])[0]), bins_to_skip))\n",
    "    # Don't plot the smallest bins since they are not interesting and dwarf all other bins\n",
    "    plt.stairs(counts[bins_to_skip:], bins[bins_to_skip:])\n",
    "    plt.title('Histogram of Token Probabilities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_most_likely_tokens(probs, num_tokens_to_plot=10):\n",
    "    ind = np.argpartition(probs, -num_tokens_to_plot)[-num_tokens_to_plot:]\n",
    "\n",
    "    likely_tokens = []\n",
    "    token_probs = []\n",
    "\n",
    "    for token in ind:\n",
    "        c_token = [(ctypes.c_int)(*[token])]\n",
    "        print('Token: {}, Prob: {}'.format(LLM.detokenize(c_token), probs[token]))\n",
    "        # print('Token: {} Detokenized: {}'.format(token, LLM.detokenize(c_token)))\n",
    "        likely_tokens.append(LLM.detokenize(c_token))\n",
    "        token_probs.append(probs[token])\n",
    "\n",
    "    plt.bar(likely_tokens, token_probs)\n",
    "    plt.title('Bar chart of 10 most likely tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "You are to act as a verbal command parser on the bridge of a star ship from the Star Trek series.\n",
      "You will be given a command spoken by the captain of the ship. \n",
      "\n",
      "You must determine which command is meant by the input. The possible outputs are [SET HEADING, SET SPEED, REVERSE COURSE, UNKNOWN]\n",
      "\n",
      "If the command is nonsensical, unclear, or null, then output \"UNKNOWN\".\n",
      "\n",
      "Respond with the command most appropriate command for the given input.\n",
      "\n",
      "### Input:\n",
      "Jelly set bongus heading.\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_logits_and_probs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/media/captdishwasher/Samshmung/horenbergerb/llama/llamagotchi/star_trek/command_parser.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/llamagotchi/star_trek/command_parser.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39m### Instruction:\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/llamagotchi/star_trek/command_parser.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mYou are to act as a verbal command parser on the bridge of a star ship from the Star Trek series.\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/llamagotchi/star_trek/command_parser.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mYou will be given a command spoken by the captain of the ship. \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/llamagotchi/star_trek/command_parser.ipynb#W6sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m### Response:\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/llamagotchi/star_trek/command_parser.ipynb#W6sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m\"\"\"\u001b[39m\u001b[39m.\u001b[39mformat(command)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/llamagotchi/star_trek/command_parser.ipynb#W6sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mprint\u001b[39m(prompt)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/llamagotchi/star_trek/command_parser.ipynb#W6sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m logits, probs \u001b[39m=\u001b[39m get_logits_and_probs(prompt)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/llamagotchi/star_trek/command_parser.ipynb#W6sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m plot_logits(logits\u001b[39m=\u001b[39mlogits)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/llamagotchi/star_trek/command_parser.ipynb#W6sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_logits_and_probs' is not defined"
     ]
    }
   ],
   "source": [
    "#command = \"Don't do anything, John!\"\n",
    "#command = \"John, beegle freegle, would ya?\"\n",
    "#command = \"John, set the heading to 150 mark 190.\"\n",
    "#command = \"Bring us around, John! Reverse course!\"\n",
    "#command = \"Bring us to full stop, John.\"\n",
    "#command = \"Full stop, John\"\n",
    "#command = \"I love jelly!\"\n",
    "#command = \"John, I love jelly!\"\n",
    "command = \"Set the jelly, John!\"\n",
    "command = \"Jelly set bongus heading.\"\n",
    "\n",
    "prompt = \"\"\"### Instruction:\n",
    "You are to act as a verbal command parser on the bridge of a star ship from the Star Trek series.\n",
    "You will be given a command spoken by the captain of the ship. \n",
    "\n",
    "You must determine which command is meant by the input. The possible outputs are [SET HEADING, SET SPEED, UNKNOWN]\n",
    "\n",
    "If the command is nonsensical, unclear, or null, then output \\\"UNKNOWN\\\".\n",
    "\n",
    "Respond with the command most appropriate command for the given input.\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "\"\"\".format(command)\n",
    "print(prompt)\n",
    "logits, probs = get_logits_and_probs(prompt)\n",
    "plot_logits(logits=logits)\n",
    "plt.show()\n",
    "plot_probs(probs)\n",
    "plt.show()\n",
    "plot_most_likely_tokens(probs)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
