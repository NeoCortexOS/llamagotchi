{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import ctypes\n",
    "import torch\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /media/captdishwasher/Samshmung/horenbergerb/llama/llama.cpp/models/WizardLM-13B-Uncensored-Q5_1-GGML/WizardML-Unc-13b-Q5_1.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32001\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 9 (mostly Q5_1)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 9534512.58 KB\n",
      "llama_model_load_internal: mem required  = 11359.05 MB (+ 1608.00 MB per state)\n",
      "..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing CLBlast (First Run)...\n",
      "Attempting to use: Platform=0, Device=0 (If invalid, program will crash)\n",
      "Using Platform: NVIDIA CUDA Device: NVIDIA GeForce RTX 3060 Ti\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".................................................................................................\n",
      "llama_init_from_file: kv self size  = 1600.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "model_bin = \"/media/captdishwasher/Samshmung/horenbergerb/llama/llama.cpp/models/WizardLM-13B-Uncensored-Q5_1-GGML/WizardML-Unc-13b-Q5_1.bin\"\n",
    "\n",
    "LLM = Llama(model_path=model_bin, n_ctx=2048, n_threads=7, use_mmap=False, n_batch=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits_and_probs(prompt):\n",
    "    tokens = LLM.tokenize(prompt.encode('utf-8'))\n",
    "    LLM.reset()\n",
    "    LLM.eval(tokens)\n",
    "    logits = LLM.eval_logits\n",
    "    logits = np.array(logits[0])\n",
    "    probs=F.softmax(torch.from_numpy(logits)).numpy()\n",
    "    return logits, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_likely_token(probs):\n",
    "    ind = np.argmax(probs)\n",
    "    c_token = [(ctypes.c_int)(*[ind])]\n",
    "    return LLM.detokenize(c_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_statement(statement):\n",
    "    prompt = \"\"\"I am going to give you a statement, and you are going to classify it. The possible classifications are Remark, Question, and Command. Respond \"Classification: X\" where X is the most likely classification. The input statement is \"{}\"\n",
    "\n",
    "    ### Response:Classification:\"\"\".format(statement)\n",
    "    logits, probs = get_logits_and_probs(prompt)\n",
    "    # ' Question' or ' Command' or ' Rem'\n",
    "    return get_most_likely_token(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_command(command):\n",
    "    prompt = \"\"\"I am going to give you a command, and you are going to classify it. The possible classifications are Heading, Speed, and Unknown. Respond \"Classification: X\" where X is the most likely classification. The input command is \"{}\"\n",
    "\n",
    "### Response:Classification:\"\"\".format(command)\n",
    "    logits, probs = get_logits_and_probs(prompt)\n",
    "    # ' Un' or ' Speed' or ' Head'\n",
    "    return get_most_likely_token(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_heading(command):\n",
    "    print(\"Command to set heading received\")\n",
    "\n",
    "def set_speed(command):\n",
    "    print(\"Command to set speed received\")\n",
    "\n",
    "def handle_unknown_command(command):\n",
    "    print(\"Unknown command recieved\")\n",
    "\n",
    "def handle_question(question):\n",
    "    prompt = \"\"\"I am going to give you a question, and you are going to make up a sci-fi, Star Trek themed response. Respond \"Response: X\" where X is the response you made up. The input question is \"{}\"\n",
    "\n",
    "### Response:Reponse:\"\"\".format(question)\n",
    "    tokens = LLM.tokenize(prompt.encode('utf-8'))\n",
    "    count = 0\n",
    "    for token in LLM.generate(tokens, top_k=40, top_p=0.8, temp=1.2, repeat_penalty=1.1):\n",
    "        print(LLM.detokenize([token]).decode(\"utf-8\") , end='')\n",
    "        count += 1\n",
    "        if count >= 200:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#command = \"Don't do anything, John!\"\n",
    "#command = \"John, beegle freegle, would ya?\"\n",
    "#command = \"John, set the heading to 150 mark 190.\"\n",
    "#command = \"Bring us around, John! Reverse course!\"\n",
    "#command = \"We've got our hands full today.\"\n",
    "#command = \"Bring us to full stop, John.\"\n",
    "#command = \"Full stop, John.\"\n",
    "#command = \"I love jelly!\"\n",
    "#command = \"John, I love jelly!\"\n",
    "#command = \"Set the jelly, John!\"\n",
    "#command = \"Jelly set bongus heading.\"\n",
    "#command = \"Warp 3. Engage.\"\n",
    "command = \"What's our current heading?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am going to give you a statement, and you are going to classify it. The possible classifications are Remark, Question, and Command. Respond \"Classification: X\" where X is the most likely classification. The input statement is \"What's our current heading?\"\n",
      "\n",
      "### Response:Classification:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2236868/2195467913.py:7: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs=F.softmax(torch.from_numpy(logits)).numpy()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b' Question'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"I am going to give you a statement, and you are going to classify it. The possible classifications are Remark, Question, and Command. Respond \"Classification: X\" where X is the most likely classification. The input statement is \"{}\"\n",
    "\n",
    "### Response:Classification:\"\"\".format(command)\n",
    "print(prompt)\n",
    "logits, probs = get_logits_and_probs(prompt)\n",
    "# ' Question' or ' Command' or ' Rem'\n",
    "classification = get_most_likely_token(probs)\n",
    "classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not a command\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2236868/2195467913.py:7: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs=F.softmax(torch.from_numpy(logits)).numpy()\n"
     ]
    }
   ],
   "source": [
    "classification = classify_statement(command)\n",
    "if classification == ' Command':\n",
    "    command_classification = classify_command(command)\n",
    "    if command_classification == ' Un':\n",
    "        handle_unknown_command(command)\n",
    "    elif command_classification == ' Speed':\n",
    "        set_speed(command)\n",
    "    elif command_classification == ' Head':\n",
    "        set_heading(command)\n",
    "    print(command_classification)\n",
    "else:\n",
    "    print('Not a command')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
