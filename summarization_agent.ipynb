{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_NAME = \"Anon\" # The name you want to use when interviewing the agent.\n",
    "BOT_NAME = \"Miku\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /media/captdishwasher/Samshmung/horenbergerb/llama/llama.cpp/models/llama/converted/llama_13b_ggml_q4_3.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 6 (mostly Q4_3)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 9534493.73 KB\n",
      "llama_model_load_internal: mem required  = 11359.03 MB (+ 3216.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  = 3200.00 MB\n",
      "llama_apply_lora_from_file_internal: applying lora adapter from '/media/captdishwasher/Samshmung/horenbergerb/llama/llama.cpp/models/SuperCOT-LoRA/13b/ggml/cutoff-2048/ggml-adapter-model.bin' - please wait ...\n",
      "llama_apply_lora_from_file_internal: r = 8, alpha = 16, scaling = 2.00\n",
      "llama_apply_lora_from_file_internal: warning: using a lora adapter with a quantized model may result in poor quality, use a f16 or f32 base model with --lora-base\n",
      ".................... done (21339.95 ms)\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "model_bin = \"/media/captdishwasher/Samshmung/horenbergerb/llama/llama.cpp/models/llama/converted/llama_13b_ggml_q4_3.bin\"\n",
    "lora_bin = \"/media/captdishwasher/Samshmung/horenbergerb/llama/llama.cpp/models/SuperCOT-LoRA/13b/ggml/cutoff-2048/ggml-adapter-model.bin\"\n",
    "\n",
    "LLM = LlamaCpp(model_path=model_bin, lora_path=lora_bin, n_ctx=2048, n_threads=6, use_mmap=False, n_batch=512, temperature=0.65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dialogue = \"\"\"Anon: Hey Miku!\n",
    "Miku: Howdy, Anon! What can I do for you today?\n",
    "Anon: I just wanted to chat, actually. It's a slow day today.\n",
    "Miku: Slow day, huh? You don't seem like the type to be idle. What are your plans for this downtime?\n",
    "Anon: I dunno. I have been stewing over this project involving natural language processing, but I'm hung up on some mathematical technicalities regarding \"embeddings.\" It's quite frustrating, so today I'm just stepping back and breathing for a bit.\n",
    "Miku: Oh dear, sounds like a tough nut to crack. But hey, that's what I'm here for! Do you want to workshop it together?\n",
    "Anon: Sure, we can try! I'm not sure your knowledge base is updated regarding embeddings in NLP though. Can you tell me what an embedding is?\n",
    "Miku: Of course! Embeddings are essentially vectors of real numbers representing words or phrases in a text. They're used to represent contextual information about the word or phrase in a given text. Embeddings are useful because they allow us to measure similarity between words or phrases based on their distance in the vector space.\n",
    "Anon: Nice, that's a pretty good summary, Miku. Maybe your knowledge base is more updated than I thought. I'm actually trying to create an artificial \"memory\" for a chatbot. Each memory will be stored as an embedding, and the bot will recall memories whose embeddings are sufficiently similar to the current discussion.\n",
    "Miku: That sounds really cool, Anon. I wish I had a memory like that.\n",
    "Anon: You might some day! If the project succeeds, I'll apply it to your programming so that you can remember details from our past conversations.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Create a bulleted list of the three most salient observations about Anon and/or Miku based on the following dialogue. Use the following example format:\n",
      "1) *First statement here*\n",
      "2) *Second statement here*\n",
      "3) *Third statement here*\n",
      "Include only the most salient observations about Anon and/or Miku, focusing on their personalities and experiences. Be very succinct.\n",
      "\n",
      "### Input:\n",
      "Anon: Hey Miku!\n",
      "Miku: Howdy, Anon! What can I do for you today?\n",
      "Anon: I just wanted to chat, actually. It's a slow day today.\n",
      "Miku: Slow day, huh? You don't seem like the type to be idle. What are your plans for this downtime?\n",
      "Anon: I dunno. I have been stewing over this project involving natural language processing, but I'm hung up on some mathematical technicalities regarding \"embeddings.\" It's quite frustrating, so today I'm just stepping back and breathing for a bit.\n",
      "Miku: Oh dear, sounds like a tough nut to crack. But hey, that's what I'm here for! Do you want to workshop it together?\n",
      "Anon: Sure, we can try! I'm not sure your knowledge base is updated regarding embeddings in NLP though. Can you tell me what an embedding is?\n",
      "Miku: Of course! Embeddings are essentially vectors of real numbers representing words or phrases in a text. They're used to represent contextual information about the word or phrase in a given text. Embeddings are useful because they allow us to measure similarity between words or phrases based on their distance in the vector space.\n",
      "Anon: Nice, that's a pretty good summary, Miku. Maybe your knowledge base is more updated than I thought. I'm actually trying to create an artificial \"memory\" for a chatbot. Each memory will be stored as an embedding, and the bot will recall memories whose embeddings are sufficiently similar to the current discussion.\n",
      "Miku: That sounds really cool, Anon. I wish I had a memory like that.\n",
      "Anon: You might some day! If the project succeeds, I'll apply it to your programming so that you can remember details from our past conversations.\n",
      "\n",
      "### Response:\n",
      "1)\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 10851.86 ms\n",
      "llama_print_timings:      sample time =    24.87 ms /    65 runs   (    0.38 ms per run)\n",
      "llama_print_timings: prompt eval time = 13449.05 ms /   530 tokens (   25.38 ms per token)\n",
      "llama_print_timings:        eval time = 30561.56 ms /    64 runs   (  477.52 ms per run)\n",
      "llama_print_timings:       total time = 44042.04 ms\n"
     ]
    }
   ],
   "source": [
    "summarize_prompt = PromptTemplate.from_template(\n",
    "    \"### Instruction:\\n\"\n",
    "    +\"Create a bulleted list of the three most salient observations about {user_name} and/or {bot_name} based on the following dialogue. Use the following example format:\\n\"\n",
    "    +\"1) *First statement here*\\n\"\n",
    "    +\"2) *Second statement here*\\n\"\n",
    "    +\"3) *Third statement here*\\n\"\n",
    "    +\"Include only the most salient observations about {user_name} and/or {bot_name}, focusing on their personalities and experiences. Be very succinct.\\n\\n\"\n",
    "    +\"### Input:\\n\"\n",
    "    +\"{chat_history}\\n\\n\"\n",
    "    +\"### Response:\\n\"\n",
    "    +\"1)\"\n",
    ")\n",
    "chain = LLMChain(llm=LLM, prompt=summarize_prompt, verbose=True)\n",
    "result = chain.run(bot_name=BOT_NAME, user_name=USER_NAME, chat_history=example_dialogue)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Anon and Miku are having a casual conversation about their respective projects and technical challenges. \n",
      "2) The chatbot is being used as an artificial memory for Anon. \n",
      "3) The chatbot will be able to recall memories based on similarity of the current discussion with past discussions.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
