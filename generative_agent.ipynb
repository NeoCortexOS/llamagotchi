{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use termcolor to make it easy to colorize the outputs.\n",
    "!pip install termcolor > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "from typing import List, Optional, Tuple\n",
    "from termcolor import colored\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain import LLMChain\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.retrievers import TimeWeightedVectorStoreRetriever\n",
    "from langchain.schema import BaseLanguageModel, Document\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /media/captdishwasher/Samshmung/horenbergerb/llama/llama.cpp/models/llama-13b-ggml-q4_0/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 7945693.73 KB\n",
      "llama_model_load_internal: mem required  = 9807.47 MB (+ 3216.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  = 3200.00 MB\n",
      "llama_apply_lora_from_file_internal: applying lora adapter from '/media/captdishwasher/Samshmung/horenbergerb/llama/llama.cpp/models/SuperCOT-LoRA/13b/ggml/cutoff-2048/ggml-adapter-model.bin' - please wait ...\n",
      "llama_apply_lora_from_file_internal: r = 8, alpha = 16, scaling = 2.00\n",
      "llama_apply_lora_from_file_internal: warning: using a lora adapter with a quantized model may result in poor quality, use a f16 or f32 base model with --lora-base\n",
      ".................... done (20619.47 ms)\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "USER_NAME = \"Beau\" # The name you want to use when interviewing the agent.\n",
    "    \n",
    "model_bin = \"/media/captdishwasher/Samshmung/horenbergerb/llama/llama.cpp/models/llama-13b-ggml-q4_0/ggml-model-q4_0.bin\"\n",
    "lora_bin = \"/media/captdishwasher/Samshmung/horenbergerb/llama/llama.cpp/models/SuperCOT-LoRA/13b/ggml/cutoff-2048/ggml-adapter-model.bin\"\n",
    "\n",
    "LLM = LlamaCpp(model_path=model_bin, lora_path=lora_bin, n_ctx=2048, n_threads=7, use_mmap=False, n_batch=512, temperature=0.62, top_p=0.4, stop=['\\n\\n### Instruction:'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"         \n",
    "         \"### Instruction:\\n\"\n",
    "         + \"On the scale of 1 to 10, where 1 is purely mundane\"\n",
    "         + \" (e.g., brushing teeth, making bed) and 10 is\"\n",
    "         + \" extremely poignant (e.g., a break up, college\"\n",
    "         + \" acceptance), rate the likely poignancy of the\"\n",
    "         + \" following piece of memory. Respond with a single integer.\\n\\n\"\n",
    "         + \"### Input:\\n\"\n",
    "         + \"Memory: {memory_content}\\n\\n\"\n",
    "         + \"### Response:\\n\"\n",
    "         + \"Rating: \"\n",
    "\"\"\"\n",
    "\n",
    "rating_prompt = \"\"\"### Instruction:\n",
    "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
    "\n",
    "### Input:\n",
    "Memory: Tommie remembers brushing his teeth\n",
    "\n",
    "### Response:\n",
    "Rating: 1\n",
    "\n",
    "### Instruction:\n",
    "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
    "\n",
    "### Input:\n",
    "Memory: {memory_content}\n",
    "\n",
    "### Response:\n",
    "Rating:\"\"\"\n",
    "\n",
    "summary_prompt = \"\"\"### Instruction:\n",
    "How would you summarize Marvin's core characteristics given the provided statements? Do not embellish. Ignore any inputs which are irrelevant to Marvin's core characteristics. Answer with a few short sentences.\n",
    "\n",
    "### Input:\n",
    "Statements:\n",
    "Marvin went to the gym\n",
    "Marvin is having Italian for dinner again\n",
    "The refrigerator is messy.\n",
    "Marvin watches TV to relax\n",
    "Marvin misses his wife.\n",
    "\n",
    "### Response:\n",
    "Marvin is a well-adjusted man with an interest in fitness. He enjoys simple acts such as watching TV. He is not particularly clean. Marvin enjoys his relationship with his wife.\n",
    "\n",
    "### Instruction:\n",
    "How would you summarize {name}'s core characteristics given the provided statements? Do not embellish. Ignore any inputs which are irrelevant to {name}'s core characteristics. Answer with a few short sentences.\n",
    "\n",
    "### Input:\n",
    "Statements:\n",
    "{related_memories}\n",
    "\n",
    "### Response:\\n\"\"\"\n",
    "\n",
    "entity_prompt = \"\"\"### Instruction:\n",
    "What/who is the subject in the provided observation?\n",
    "\n",
    "### Input:\n",
    "The dog is chasing a ball.\n",
    "\n",
    "### Response:\n",
    "The dog\n",
    "\n",
    "### Instruction:\n",
    "What/who is the subject in the provided observation?\n",
    "\n",
    "### Input:\n",
    "{observation}\n",
    "\n",
    "### Response:\\n\"\"\"\n",
    "\n",
    "entity_action_prompt = \"\"\"### Instruction:\n",
    "What is The dog doing in the provided observation?\n",
    "\n",
    "### Input:\n",
    "The dog jumps into the air and catches the ball.\n",
    "\n",
    "### Response:\n",
    "Catching a ball, possibly playing fetch\n",
    "\n",
    "### Instruction:\n",
    "What is {entity} doing in the provided observation?\n",
    "\n",
    "### Input:\n",
    "{observation}\n",
    "\n",
    "### Response:\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeAgent(BaseModel):\n",
    "    \"\"\"A character with memory and innate characteristics.\"\"\"\n",
    "    \n",
    "    name: str\n",
    "    age: int\n",
    "    traits: str\n",
    "    \"\"\"The traits of the character you wish not to change.\"\"\"\n",
    "    status: str\n",
    "    \"\"\"Current activities of the character.\"\"\"\n",
    "    llm: BaseLanguageModel\n",
    "    memory_retriever: TimeWeightedVectorStoreRetriever\n",
    "    \"\"\"The retriever to fetch related memories.\"\"\"\n",
    "    verbose: bool = False\n",
    "    \n",
    "    reflection_threshold: Optional[float] = None\n",
    "    \"\"\"When the total 'importance' of memories exceeds the above threshold, stop to reflect.\"\"\"\n",
    "    \n",
    "    current_plan: List[str] = []\n",
    "    \"\"\"The current plan of the agent.\"\"\"\n",
    "    \n",
    "    summary: str = \"\"  #: :meta private:\n",
    "    summary_refresh_seconds: int= 3600  #: :meta private:\n",
    "    last_refreshed: datetime =Field(default_factory=datetime.now)  #: :meta private:\n",
    "    daily_summaries: List[str] #: :meta private:\n",
    "    memory_importance: float = 0.0 #: :meta private:\n",
    "    max_tokens_limit: int = 1200 #: :meta private:\n",
    "    \n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_list(text: str) -> List[str]:\n",
    "        \"\"\"Parse a newline-separated string into a list of strings.\"\"\"\n",
    "        lines = re.split(r'\\n', text.strip())\n",
    "        return [re.sub(r'^\\s*\\d+\\.\\s*', '', line).strip() for line in lines]\n",
    "\n",
    "\n",
    "    def _compute_agent_summary(self):\n",
    "        \"\"\"\"\"\"\n",
    "        prompt = PromptTemplate.from_template(summary_prompt)\n",
    "        # The agent seeks to think about their core characteristics.\n",
    "        relevant_memories = self.fetch_memories(f\"{self.name}'s core characteristics\")\n",
    "        relevant_memories_str = \"\\n\".join([f\"{mem.page_content}\" for mem in relevant_memories])\n",
    "        chain = LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)\n",
    "        return chain.run(name=self.name, related_memories=relevant_memories_str).strip()\n",
    "    \n",
    "    def _get_topics_of_reflection(self, last_k: int = 50) -> Tuple[str, str, str]:\n",
    "        \"\"\"Return the 3 most salient high-level questions about recent observations.\"\"\"\n",
    "        prompt = PromptTemplate.from_template(\n",
    "            \"### Instruction:\\n\"\n",
    "            + \"Given only the provided information, what are the 3 most salient\"\n",
    "            + \" high-level questions we can answer about the subjects in the statements?\"\n",
    "            + \" Provide each question on a new line.\\n\\n\"\n",
    "            + \"### Input:\\n\"\n",
    "            + \"{observations}\\n\\n\"\n",
    "            + \"### Response:\\n\"\n",
    "        )\n",
    "        reflection_chain = LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)\n",
    "        observations = self.memory_retriever.memory_stream[-last_k:]\n",
    "        observation_str = \"\\n\".join([o.page_content for o in observations])\n",
    "        result = reflection_chain.run(observations=observation_str)\n",
    "        return self._parse_list(result)\n",
    "    \n",
    "    def _get_insights_on_topic(self, topic: str) -> List[str]:\n",
    "        \"\"\"Generate 'insights' on a topic of reflection, based on pertinent memories.\"\"\"\n",
    "        prompt = PromptTemplate.from_template(\n",
    "            \"### Instruction:\\n\"\n",
    "            + \"What 5 high-level insights can you infer from the provided statements?\"\n",
    "            + \" (example format: insight (because of 1, 5, 3))\\n\\n\"\n",
    "            + \"### Input:\\n\"\n",
    "            + \"Statements about {topic}\\n\"\n",
    "            + \"{related_statements}\\n\\n\"\n",
    "            + \"### Response:\\n\"\n",
    "        )\n",
    "        related_memories = self.fetch_memories(topic)\n",
    "        related_statements = \"\\n\".join([f\"{i+1}. {memory.page_content}\" \n",
    "                                        for i, memory in \n",
    "                                        enumerate(related_memories)])\n",
    "        reflection_chain = LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)\n",
    "        result = reflection_chain.run(topic=topic, related_statements=related_statements)\n",
    "        # TODO: Parse the connections between memories and insights\n",
    "        return self._parse_list(result)\n",
    "    \n",
    "    def pause_to_reflect(self) -> List[str]:\n",
    "        \"\"\"Reflect on recent observations and generate 'insights'.\"\"\"\n",
    "        print(colored(f\"Character {self.name} is reflecting\", \"blue\"))\n",
    "        new_insights = []\n",
    "        topics = self._get_topics_of_reflection()\n",
    "        for topic in topics:\n",
    "            insights = self._get_insights_on_topic( topic)\n",
    "            for insight in insights:\n",
    "                self.add_memory(insight)\n",
    "            new_insights.extend(insights)\n",
    "        return new_insights\n",
    "    \n",
    "    def _score_memory_importance(self, memory_content: str, weight: float = 0.15) -> float:\n",
    "        \"\"\"Score the absolute importance of the given memory.\"\"\"\n",
    "        # A weight of 0.25 makes this less important than it\n",
    "        # would be otherwise, relative to salience and time\n",
    "        prompt = PromptTemplate.from_template(rating_prompt)\n",
    "        chain = LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)\n",
    "        score = chain.run(memory_content=memory_content).strip()\n",
    "        match = re.search(r\"^\\D*(\\d+)\", score)\n",
    "        if match:\n",
    "            return (float(score[0]) / 10) * weight\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "\n",
    "    def add_memory(self, memory_content: str) -> List[str]:\n",
    "        \"\"\"Add an observation or memory to the agent's memory.\"\"\"\n",
    "        importance_score = self._score_memory_importance(memory_content)\n",
    "        print(importance_score)\n",
    "        self.memory_importance += importance_score\n",
    "        document = Document(page_content=memory_content, metadata={\"importance\": importance_score})\n",
    "        result = self.memory_retriever.add_documents([document])\n",
    "\n",
    "        # After an agent has processed a certain amount of memories (as measured by\n",
    "        # aggregate importance), it is time to reflect on recent events to add\n",
    "        # more synthesized memories to the agent's memory stream.\n",
    "        if (self.reflection_threshold is not None \n",
    "            and self.memory_importance > self.reflection_threshold\n",
    "            and self.status != \"Reflecting\"):\n",
    "            old_status = self.status\n",
    "            self.status = \"Reflecting\"\n",
    "            self.pause_to_reflect()\n",
    "            # Hack to clear the importance from reflection\n",
    "            self.memory_importance = 0.0\n",
    "            self.status = old_status\n",
    "        return result\n",
    "    \n",
    "    def fetch_memories(self, observation: str) -> List[Document]:\n",
    "        \"\"\"Fetch related memories.\"\"\"\n",
    "        return self.memory_retriever.get_relevant_documents(observation)\n",
    "    \n",
    "        \n",
    "    def get_summary(self, force_refresh: bool = False) -> str:\n",
    "        \"\"\"Return a descriptive summary of the agent.\"\"\"\n",
    "        current_time = datetime.now()\n",
    "        since_refresh = (current_time - self.last_refreshed).seconds\n",
    "        if not self.summary or since_refresh >= self.summary_refresh_seconds or force_refresh:\n",
    "            self.summary = self._compute_agent_summary()\n",
    "            self.last_refreshed = current_time\n",
    "        return (\n",
    "            f\"Name: {self.name} (age: {self.age})\"\n",
    "            +f\"\\nInnate traits: {self.traits}\"\n",
    "            +f\"\\n{self.summary}\"\n",
    "        )\n",
    "    \n",
    "    def get_full_header(self, force_refresh: bool = False) -> str:\n",
    "        \"\"\"Return a full header of the agent's status, summary, and current time.\"\"\"\n",
    "        summary = self.get_summary(force_refresh=force_refresh)\n",
    "        current_time_str =  datetime.now().strftime(\"%B %d, %Y, %I:%M %p\")\n",
    "        return f\"{summary}\\nIt is {current_time_str}.\\n{self.name}'s status: {self.status}\"\n",
    "\n",
    "    \n",
    "    \n",
    "    def _get_entity_from_observation(self, observation: str) -> str:\n",
    "        prompt = PromptTemplate.from_template(entity_prompt)\n",
    "        chain = LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)\n",
    "        return chain.run(observation=observation).strip()\n",
    "\n",
    "    def _get_entity_action(self, observation: str, entity_name: str) -> str:\n",
    "        prompt = PromptTemplate.from_template(entity_action_prompt)\n",
    "        chain = LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)\n",
    "        return chain.run(entity=entity_name, observation=observation).strip()\n",
    "    \n",
    "    def _format_memories_to_summarize(self, relevant_memories: List[Document]) -> str:\n",
    "        content_strs = set()\n",
    "        content = []\n",
    "        for mem in relevant_memories:\n",
    "            if mem.page_content in content_strs:\n",
    "                continue\n",
    "            content_strs.add(mem.page_content)\n",
    "            created_time = mem.metadata[\"created_at\"].strftime(\"%B %d, %Y, %I:%M %p\")\n",
    "            content.append(f\"- {created_time}: {mem.page_content.strip()}\")\n",
    "        return \"\\n\".join([f\"{mem}\" for mem in content])\n",
    "    \n",
    "    def summarize_related_memories(self, observation: str) -> str:\n",
    "        \"\"\"Summarize memories that are most relevant to an observation.\"\"\"\n",
    "        entity_name = self._get_entity_from_observation(observation)\n",
    "        entity_action = self._get_entity_action(observation, entity_name)\n",
    "        q1 = f\"### Instruction:\\nGiven the provided information, what is the relationship between {self.name} and {entity_name}? Be succinct.\"\n",
    "        relevant_memories = self.fetch_memories(q1) # Fetch memories related to the agent's relationship with the entity\n",
    "        q2 = f\"{entity_name} is {entity_action}\"\n",
    "        relevant_memories += self.fetch_memories(q2) # Fetch things related to the entity-action pair\n",
    "        context_str = self._format_memories_to_summarize(relevant_memories)\n",
    "        prompt = PromptTemplate.from_template(\n",
    "            \"{q1}\\n\\n### Input:\\nContext from memory:\\n{context_str}\\n\\n### Response:\\n\"\n",
    "        )\n",
    "        chain = LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)\n",
    "        return chain.run(q1=q1, context_str=context_str.strip()).strip()\n",
    "    \n",
    "    def _get_memories_until_limit(self, consumed_tokens: int) -> str:\n",
    "        \"\"\"Reduce the number of tokens in the documents.\"\"\"\n",
    "        result = []\n",
    "        for doc in self.memory_retriever.memory_stream[::-1]:\n",
    "            if consumed_tokens >= self.max_tokens_limit:\n",
    "                break\n",
    "            consumed_tokens += self.llm.get_num_tokens(doc.page_content)\n",
    "            if consumed_tokens < self.max_tokens_limit:\n",
    "                result.append(doc.page_content) \n",
    "        return \"; \".join(result[::-1])\n",
    "    \n",
    "    def _generate_reaction(\n",
    "        self,\n",
    "        observation: str,\n",
    "        task: str\n",
    "    ) -> str:\n",
    "        \"\"\"React to a given observation.\"\"\"\n",
    "        prompt = PromptTemplate.from_template(\n",
    "                \"### Instruction:\\n\"\n",
    "                + task + \"\\n\\n\"\n",
    "                + \"### Input:\\n\"\n",
    "                + \"{agent_summary_description}\"\n",
    "                +\"\\nIt is {current_time}.\"\n",
    "                +\"\\n{agent_name}'s status: {agent_status}\"\n",
    "                + \"\\nSummary of relevant context from {agent_name}'s memory:\"\n",
    "                +\"\\n{relevant_memories}\"\n",
    "                +\"\\nMost recent observations: {recent_observations}\"\n",
    "                + \"\\nObservation: {observation}\\n\\n\"\n",
    "                + \"### Response:\\n\"\n",
    "        )\n",
    "        agent_summary_description = self.get_summary()\n",
    "        relevant_memories_str = self.summarize_related_memories(observation)\n",
    "        current_time_str = datetime.now().strftime(\"%B %d, %Y, %I:%M %p\")\n",
    "        kwargs = dict(agent_summary_description=agent_summary_description,\n",
    "                      current_time=current_time_str,\n",
    "                      relevant_memories=relevant_memories_str,\n",
    "                      agent_name=self.name,\n",
    "                      observation=observation,\n",
    "                     agent_status=self.status)\n",
    "        consumed_tokens = self.llm.get_num_tokens(prompt.format(recent_observations=\"\", **kwargs))\n",
    "        kwargs[\"recent_observations\"] = self._get_memories_until_limit(consumed_tokens)\n",
    "        action_prediction_chain = LLMChain(llm=self.llm, prompt=prompt, verbose=True)\n",
    "        result = action_prediction_chain.run(**kwargs)\n",
    "        return result.strip()\n",
    "    \n",
    "    def generate_reaction(self, observation: str) -> Tuple[bool, str]:\n",
    "        \"\"\"React to a given observation.\"\"\"\n",
    "        call_to_action_template = (\n",
    "            \"Should {agent_name} react to the observation, and if so,\"\n",
    "            + \" what would be an appropriate reaction? Respond in one line.\"\n",
    "            + ' If the action is to engage in dialogue, write:\\nSAY: \"what to say\"'\n",
    "            + \"\\notherwise, write:\\nREACT: {agent_name}'s reaction (if anything).\"\n",
    "            + \"\\nEither do nothing, react, or say something but not both.\"\n",
    "        )\n",
    "        full_result = self._generate_reaction(observation, call_to_action_template)\n",
    "        result = full_result.strip().split('\\n')[0]\n",
    "        self.add_memory(f\"{self.name} observed {observation} and reacted by {result}\")\n",
    "        if \"REACT:\" in result:\n",
    "            reaction = result.split(\"REACT:\")[-1].strip()\n",
    "            return False, f\"{self.name} {reaction}\"\n",
    "        if \"SAY:\" in result:\n",
    "            said_value = result.split(\"SAY:\")[-1].strip()\n",
    "            return True, f\"{self.name} said {said_value}\"\n",
    "        else:\n",
    "            return False, result\n",
    "\n",
    "    def generate_dialogue_response(self, observation: str) -> Tuple[bool, str]:\n",
    "        \"\"\"React to a given observation.\"\"\"\n",
    "        call_to_action_template = (\n",
    "            'Given the provided context, what would {agent_name} say? To end the conversation, write: GOODBYE: \"what to say\". Otherwise to continue the conversation, write: SAY: \"what to say next\"'\n",
    "        )\n",
    "        full_result = self._generate_reaction(observation, call_to_action_template)\n",
    "        result = full_result.strip().split('\\n')[0]\n",
    "        if \"GOODBYE:\" in result:\n",
    "            farewell = result.split(\"GOODBYE:\")[-1].strip()\n",
    "            self.add_memory(f\"{self.name} observed {observation} and said {farewell}\")\n",
    "            return False, f\"{self.name} said {farewell}\"\n",
    "        if \"SAY:\" in result:\n",
    "            response_text = result.split(\"SAY:\")[-1].strip()\n",
    "            self.add_memory(f\"{self.name} observed {observation} and said {response_text}\")\n",
    "            return True, f\"{self.name} said {response_text}\"\n",
    "        else:\n",
    "            return False, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /media/captdishwasher/Samshmung/horenbergerb/llama/llama.cpp/models/llama-13b-ggml-q4_0/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =  73.73 KB\n",
      "llama_model_load_internal: mem required  = 9807.47 MB (+ 3216.00 MB per state)\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | llama_init_from_file: kv self size  = 3200.00 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "def normalize(v):\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm == 0: \n",
    "       return v\n",
    "    return v / norm\n",
    "\n",
    "embeddings_model = LlamaCppEmbeddings(model_path=model_bin, n_ctx=2048, n_threads=6, n_batch=512)\n",
    "def embedding_function(query):\n",
    "    x = embeddings_model.embed_query(query)\n",
    "    return normalize(x)\n",
    "\n",
    "def relevance_score_fn(score: float) -> float:\n",
    "    \"\"\"Return a similarity score on a scale [0, 1].\"\"\"\n",
    "    # This is probably a really dumb similarity score\n",
    "    # The embeddings don't seem to have any bounds on what the vector magnitudes can be\n",
    "    # So I normalized them to be between 0 and 1\n",
    "    # Then FAISS takes the squared Euclidean distance between these vectors\n",
    "    # So the output has a max value of 4 and a minimum of 0\n",
    "    # But it needs to be between 0 and 1\n",
    "    # Hence the function:\n",
    "    return (4.0 - score) / 4\n",
    "\n",
    "def create_new_memory_retriever():\n",
    "    \"\"\"Create a new vector store retriever unique to the agent.\"\"\"\n",
    "    # Initialize the vectorstore as empty\n",
    "    embedding_size = 5120\n",
    "    index = faiss.IndexFlatL2(embedding_size)\n",
    "    vectorstore = FAISS(embedding_function, index, InMemoryDocstore({}), {}, relevance_score_fn=relevance_score_fn)\n",
    "    return TimeWeightedVectorStoreRetriever(vectorstore=vectorstore, other_score_keys=[\"importance\"], k=15)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tommie = GenerativeAgent(name=\"Tommie\", \n",
    "              age=25,\n",
    "              traits=\"anxious, likes design\", # You can add more persistent traits here \n",
    "              status=\"looking for a job\", # When connected to a virtual world, we can have the characters update their status\n",
    "              memory_retriever=create_new_memory_retriever(),\n",
    "              llm=LLM,\n",
    "              daily_summaries = [\n",
    "                   \"Drove across state to move to a new town but doesn't have a job yet.\"\n",
    "               ],\n",
    "              reflection_threshold = 8, # we will give this a relatively low number to show how reflection works\n",
    "              verbose=True\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie remembers brushing his teeth\n",
      "\n",
      "### Response:\n",
      "Rating: 1\n",
      "\n",
      "### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie remembers his dog, Bruno, from when he was a kid\n",
      "\n",
      "### Response:\n",
      "Rating:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "0.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     3.29 ms /     9 runs   (    0.37 ms per run)\n",
      "llama_print_timings: prompt eval time =  3968.94 ms /   225 tokens (   17.64 ms per token)\n",
      "llama_print_timings:        eval time =  3137.02 ms /     8 runs   (  392.13 ms per run)\n",
      "llama_print_timings:       total time =  7109.98 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  1483.33 ms /    17 tokens (   87.25 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  1483.67 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie remembers brushing his teeth\n",
      "\n",
      "### Response:\n",
      "Rating: 1\n",
      "\n",
      "### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie feels tired from driving so far\n",
      "\n",
      "### Response:\n",
      "Rating:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "0.015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     3.25 ms /     9 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time =  3977.97 ms /   218 tokens (   18.25 ms per token)\n",
      "llama_print_timings:        eval time =  3160.28 ms /     8 runs   (  395.04 ms per run)\n",
      "llama_print_timings:       total time =  7142.15 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  1116.66 ms /    10 tokens (  111.67 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  1116.87 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie remembers brushing his teeth\n",
      "\n",
      "### Response:\n",
      "Rating: 1\n",
      "\n",
      "### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie sees the new home\n",
      "\n",
      "### Response:\n",
      "Rating:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "0.015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     3.52 ms /    10 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time =  3781.20 ms /   216 tokens (   17.51 ms per token)\n",
      "llama_print_timings:        eval time =  3489.33 ms /     9 runs   (  387.70 ms per run)\n",
      "llama_print_timings:       total time =  7274.71 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =   690.47 ms /     8 tokens (   86.31 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =   690.82 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie remembers brushing his teeth\n",
      "\n",
      "### Response:\n",
      "Rating: 1\n",
      "\n",
      "### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: The new neighbors have a cat\n",
      "\n",
      "### Response:\n",
      "Rating:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "0.015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     3.23 ms /     9 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time =  3774.77 ms /   216 tokens (   17.48 ms per token)\n",
      "llama_print_timings:        eval time =  3097.89 ms /     8 runs   (  387.24 ms per run)\n",
      "llama_print_timings:       total time =  6876.45 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =   701.44 ms /     8 tokens (   87.68 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =   701.73 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie remembers brushing his teeth\n",
      "\n",
      "### Response:\n",
      "Rating: 1\n",
      "\n",
      "### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: The road is noisy at night\n",
      "\n",
      "### Response:\n",
      "Rating:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "0.015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     3.18 ms /     9 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time =  3782.37 ms /   216 tokens (   17.51 ms per token)\n",
      "llama_print_timings:        eval time =  3097.53 ms /     8 runs   (  387.19 ms per run)\n",
      "llama_print_timings:       total time =  6883.77 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =   686.14 ms /     8 tokens (   85.77 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =   686.42 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie remembers brushing his teeth\n",
      "\n",
      "### Response:\n",
      "Rating: 1\n",
      "\n",
      "### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie is hungry\n",
      "\n",
      "### Response:\n",
      "Rating:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "0.015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     3.26 ms /     9 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time =  3790.93 ms /   215 tokens (   17.63 ms per token)\n",
      "llama_print_timings:        eval time =  3089.07 ms /     8 runs   (  386.13 ms per run)\n",
      "llama_print_timings:       total time =  6883.95 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =   615.14 ms /     7 tokens (   87.88 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =   615.43 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie remembers brushing his teeth\n",
      "\n",
      "### Response:\n",
      "Rating: 1\n",
      "\n",
      "### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie tries to get some rest.\n",
      "\n",
      "### Response:\n",
      "Rating:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "0.015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     3.21 ms /     9 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time =  3831.65 ms /   218 tokens (   17.58 ms per token)\n",
      "llama_print_timings:        eval time =  3087.84 ms /     8 runs   (  385.98 ms per run)\n",
      "llama_print_timings:       total time =  6923.28 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =   842.37 ms /    10 tokens (   84.24 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =   842.51 ms\n"
     ]
    }
   ],
   "source": [
    "# We can give the character memories directly\n",
    "tommie_memories = [\n",
    "    \"Tommie remembers his dog, Bruno, from when he was a kid\",\n",
    "    \"Tommie feels tired from driving so far\",\n",
    "    \"Tommie sees the new home\",\n",
    "    \"The new neighbors have a cat\",\n",
    "    \"The road is noisy at night\",\n",
    "    \"Tommie is hungry\",\n",
    "    \"Tommie tries to get some rest.\",\n",
    "]\n",
    "try:\n",
    "    for memory in tommie_memories:\n",
    "        tommie.add_memory(memory)\n",
    "except:\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =   784.42 ms /     8 tokens (   98.05 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =   784.78 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "How would you summarize Marvin's core characteristics given the provided statements? Do not embellish. Ignore any inputs which are irrelevant to Marvin's core characteristics. Answer with a few short sentences.\n",
      "\n",
      "### Input:\n",
      "Statements:\n",
      "Marvin went to the gym\n",
      "Marvin is having Italian for dinner again\n",
      "The refrigerator is messy.\n",
      "Marvin watches TV to relax\n",
      "Marvin misses his wife.\n",
      "\n",
      "### Response:\n",
      "Marvin is a well-adjusted man with an interest in fitness. He enjoys simple acts such as watching TV. He is not particularly clean. Marvin enjoys his relationship with his wife.\n",
      "\n",
      "### Instruction:\n",
      "How would you summarize Tommie's core characteristics given the provided statements? Do not embellish. Ignore any inputs which are irrelevant to Tommie's core characteristics. Answer with a few short sentences.\n",
      "\n",
      "### Input:\n",
      "Statements:\n",
      "Tommie sees the new home\n",
      "The new neighbors have a cat\n",
      "Tommie remembers his dog, Bruno, from when he was a kid\n",
      "Tommie feels tired from driving so far\n",
      "The road is noisy at night\n",
      "Tommie is hungry\n",
      "Tommie tries to get some rest.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Name: Tommie (age: 25)\n",
      "Innate traits: anxious, likes design\n",
      "Tommie is an older man who has fond memories of his childhood pet, Bruno. He is a homebody and enjoys the comforts of home. Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =    25.16 ms /    71 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time =  4729.92 ms /   292 tokens (   16.20 ms per token)\n",
      "llama_print_timings:        eval time = 27776.73 ms /    70 runs   (  396.81 ms per run)\n",
      "llama_print_timings:       total time = 32537.07 ms\n"
     ]
    }
   ],
   "source": [
    "# Now that Tommie has 'memories', their self-summary is more descriptive, though still rudimentary.\n",
    "# We will see how this summary updates after more observations to create a more rich description.\n",
    "try:\n",
    "    print(tommie.get_summary(force_refresh=True))\n",
    "except:\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interview_agent(agent: GenerativeAgent, message: str) -> str:\n",
    "    \"\"\"Help the notebook user interact with the agent.\"\"\"\n",
    "    new_message = f\"{USER_NAME} says {message}\"\n",
    "    return agent.generate_dialogue_response(new_message)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog is chasing a ball.\n",
      "\n",
      "### Response:\n",
      "The dog\n",
      "\n",
      "### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "Beau says What do you like to do?\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What is The dog doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog jumps into the air and catches the ball.\n",
      "\n",
      "### Response:\n",
      "Catching a ball, possibly playing fetch\n",
      "\n",
      "### Instruction:\n",
      "What is Beau doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "Beau says What do you like to do?\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     3.09 ms /     9 runs   (    0.34 ms per run)\n",
      "llama_print_timings: prompt eval time =  2658.34 ms /    84 tokens (   31.65 ms per token)\n",
      "llama_print_timings:        eval time =  2992.83 ms /     8 runs   (  374.10 ms per run)\n",
      "llama_print_timings:       total time =  5654.96 ms\n",
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     7.69 ms /    22 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time =  2696.14 ms /    93 tokens (   28.99 ms per token)\n",
      "llama_print_timings:        eval time =  7920.99 ms /    21 runs   (  377.19 ms per run)\n",
      "llama_print_timings:       total time = 10626.35 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2208.28 ms /    28 tokens (   78.87 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2208.64 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  1432.22 ms /    18 tokens (   79.57 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  1432.41 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Given the provided information, what is the relationship between Tommie and Beau? Be succinct.\n",
      "\n",
      "### Input:\n",
      "Context from memory:\n",
      "- April 23, 2023, 09:51 PM: Tommie tries to get some rest.\n",
      "- April 23, 2023, 09:50 PM: Tommie remembers his dog, Bruno, from when he was a kid\n",
      "- April 23, 2023, 09:51 PM: Tommie is hungry\n",
      "- April 23, 2023, 09:51 PM: The road is noisy at night\n",
      "- April 23, 2023, 09:51 PM: The new neighbors have a cat\n",
      "- April 23, 2023, 09:50 PM: Tommie feels tired from driving so far\n",
      "- April 23, 2023, 09:50 PM: Tommie sees the new home\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =    20.10 ms /    57 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time =  4154.89 ms /   253 tokens (   16.42 ms per token)\n",
      "llama_print_timings:        eval time = 22059.85 ms /    56 runs   (  393.93 ms per run)\n",
      "llama_print_timings:       total time = 26239.33 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Given the provided context, what would Tommie say? To end the conversation, write: GOODBYE: \"what to say\". Otherwise to continue the conversation, write: SAY: \"what to say next\"\n",
      "\n",
      "### Input:\n",
      "Name: Tommie (age: 25)\n",
      "Innate traits: anxious, likes design\n",
      "Tommie is an older man who has fond memories of his childhood pet, Bruno. He is a homebody and enjoys the comforts of home. Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "It is April 23, 2023, 09:52 PM.\n",
      "Tommie's status: looking for a job\n",
      "Summary of relevant context from Tommie's memory:\n",
      "Tommie is a man who has just moved into his new house. He remembers his dog Bruno and feels hungry at night. The road is noisy at night and he notices that the neighbors have a cat. He also feels tired from driving so far.\n",
      "Most recent observations: Tommie remembers his dog, Bruno, from when he was a kid; Tommie feels tired from driving so far; Tommie sees the new home; The new neighbors have a cat; The road is noisy at night; Tommie is hungry; Tommie tries to get some rest.\n",
      "Observation: Beau says What do you like to do?\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie remembers brushing his teeth\n",
      "\n",
      "### Response:\n",
      "Rating: 1\n",
      "\n",
      "### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie observed Beau says What do you like to do? and said \"what to say\".\n",
      "\n",
      "### Response:\n",
      "Rating:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =    46.01 ms /   129 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time =  5560.55 ms /   348 tokens (   15.98 ms per token)\n",
      "llama_print_timings:        eval time = 52069.94 ms /   128 runs   (  406.80 ms per run)\n",
      "llama_print_timings:       total time = 57688.46 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "0.015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     3.34 ms /     9 runs   (    0.37 ms per run)\n",
      "llama_print_timings: prompt eval time =  3913.48 ms /   229 tokens (   17.09 ms per token)\n",
      "llama_print_timings:        eval time =  3100.40 ms /     8 runs   (  387.55 ms per run)\n",
      "llama_print_timings:       total time =  7017.81 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  1708.94 ms /    21 tokens (   81.38 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  1709.15 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Tommie said \"what to say\".'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interview_agent(tommie, \"What do you like to do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog is chasing a ball.\n",
      "\n",
      "### Response:\n",
      "The dog\n",
      "\n",
      "### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "Beau says What are you looking forward to doing today?\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What is The dog doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog jumps into the air and catches the ball.\n",
      "\n",
      "### Response:\n",
      "Catching a ball, possibly playing fetch\n",
      "\n",
      "### Instruction:\n",
      "What is You doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "Beau says What are you looking forward to doing today?\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     2.84 ms /     8 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time =  2658.20 ms /    86 tokens (   30.91 ms per token)\n",
      "llama_print_timings:        eval time =  2643.34 ms /     7 runs   (  377.62 ms per run)\n",
      "llama_print_timings:       total time =  5305.01 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     6.48 ms /    18 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time =  2735.72 ms /    95 tokens (   28.80 ms per token)\n",
      "llama_print_timings:        eval time =  6404.65 ms /    17 runs   (  376.74 ms per run)\n",
      "llama_print_timings:       total time =  9148.22 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2200.80 ms /    28 tokens (   78.60 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2201.16 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  1050.06 ms /    13 tokens (   80.77 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  1050.24 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Given the provided information, what is the relationship between Tommie and You? Be succinct.\n",
      "\n",
      "### Input:\n",
      "Context from memory:\n",
      "- April 23, 2023, 09:51 PM: Tommie tries to get some rest.\n",
      "- April 23, 2023, 09:50 PM: Tommie remembers his dog, Bruno, from when he was a kid\n",
      "- April 23, 2023, 09:51 PM: Tommie is hungry\n",
      "- April 23, 2023, 09:51 PM: The road is noisy at night\n",
      "- April 23, 2023, 09:51 PM: The new neighbors have a cat\n",
      "- April 23, 2023, 09:50 PM: Tommie feels tired from driving so far\n",
      "- April 23, 2023, 09:53 PM: Tommie observed Beau says What do you like to do? and said \"what to say\".\n",
      "- April 23, 2023, 09:50 PM: Tommie sees the new home\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =    31.24 ms /    88 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time =  4807.10 ms /   294 tokens (   16.35 ms per token)\n",
      "llama_print_timings:        eval time = 34916.68 ms /    87 runs   (  401.34 ms per run)\n",
      "llama_print_timings:       total time = 39762.86 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Given the provided context, what would Tommie say? To end the conversation, write: GOODBYE: \"what to say\". Otherwise to continue the conversation, write: SAY: \"what to say next\"\n",
      "\n",
      "### Input:\n",
      "Name: Tommie (age: 25)\n",
      "Innate traits: anxious, likes design\n",
      "Tommie is an older man who has fond memories of his childhood pet, Bruno. He is a homebody and enjoys the comforts of home. Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "It is April 23, 2023, 09:54 PM.\n",
      "Tommie's status: looking for a job\n",
      "Summary of relevant context from Tommie's memory:\n",
      "Tommie is a person who has just moved into a new house. He is tired from driving so far and observes his dog Bruno, which he had when he was a kid. He also notices that the road is noisy at night and feels hungry. Tommie also sees the new neighbors have a cat and Beau says What do you like to do? and said \"what to say\".\n",
      "```\n",
      "Most recent observations: Tommie remembers his dog, Bruno, from when he was a kid; Tommie feels tired from driving so far; Tommie sees the new home; The new neighbors have a cat; The road is noisy at night; Tommie is hungry; Tommie tries to get some rest.; Tommie observed Beau says What do you like to do? and said \"what to say\".\n",
      "Observation: Beau says What are you looking forward to doing today?\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     5.68 ms /    16 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time =  6689.39 ms /   400 tokens (   16.72 ms per token)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'```'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_print_timings:        eval time =  6112.60 ms /    15 runs   (  407.51 ms per run)\n",
      "llama_print_timings:       total time = 12808.89 ms\n"
     ]
    }
   ],
   "source": [
    "interview_agent(tommie, \"What are you looking forward to doing today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog is chasing a ball.\n",
      "\n",
      "### Response:\n",
      "The dog\n",
      "\n",
      "### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "Beau says What are you most worried about today?\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What is The dog doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog jumps into the air and catches the ball.\n",
      "\n",
      "### Response:\n",
      "Catching a ball, possibly playing fetch\n",
      "\n",
      "### Instruction:\n",
      "What is You doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "Beau says What are you most worried about today?\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     2.93 ms /     8 runs   (    0.37 ms per run)\n",
      "llama_print_timings: prompt eval time =  2687.82 ms /    86 tokens (   31.25 ms per token)\n",
      "llama_print_timings:        eval time =  2634.50 ms /     7 runs   (  376.36 ms per run)\n",
      "llama_print_timings:       total time =  5325.90 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     6.81 ms /    19 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time =  2709.77 ms /    95 tokens (   28.52 ms per token)\n",
      "llama_print_timings:        eval time =  6779.89 ms /    18 runs   (  376.66 ms per run)\n",
      "llama_print_timings:       total time =  9497.80 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2203.35 ms /    28 tokens (   78.69 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2203.64 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  1143.59 ms /    14 tokens (   81.69 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  1143.76 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Given the provided information, what is the relationship between Tommie and You? Be succinct.\n",
      "\n",
      "### Input:\n",
      "Context from memory:\n",
      "- April 23, 2023, 09:51 PM: Tommie tries to get some rest.\n",
      "- April 23, 2023, 09:50 PM: Tommie remembers his dog, Bruno, from when he was a kid\n",
      "- April 23, 2023, 09:51 PM: Tommie is hungry\n",
      "- April 23, 2023, 09:51 PM: The road is noisy at night\n",
      "- April 23, 2023, 09:51 PM: The new neighbors have a cat\n",
      "- April 23, 2023, 09:50 PM: Tommie feels tired from driving so far\n",
      "- April 23, 2023, 09:53 PM: Tommie observed Beau says What do you like to do? and said \"what to say\".\n",
      "- April 23, 2023, 09:50 PM: Tommie sees the new home\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =    90.64 ms /   256 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time =  4772.48 ms /   294 tokens (   16.23 ms per token)\n",
      "llama_print_timings:        eval time = 104165.76 ms /   255 runs   (  408.49 ms per run)\n",
      "llama_print_timings:       total time = 109059.28 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Given the provided context, what would Tommie say? To end the conversation, write: GOODBYE: \"what to say\". Otherwise to continue the conversation, write: SAY: \"what to say next\"\n",
      "\n",
      "### Input:\n",
      "Name: Tommie (age: 25)\n",
      "Innate traits: anxious, likes design\n",
      "Tommie is an older man who has fond memories of his childhood pet, Bruno. He is a homebody and enjoys the comforts of home. Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "It is April 23, 2023, 09:57 PM.\n",
      "Tommie's status: looking for a job\n",
      "Summary of relevant context from Tommie's memory:\n",
      "Tommie is a person. You are a person. Tommie is talking about himself in the first person perspective and you are listening to him talk about himself.\n",
      "\n",
      "### Output:\n",
      "You are listening to Tommie talk about himself in the first person perspective.\n",
      "```\n",
      "## Instruction:\n",
      "Given the provided information, what is the relationship between Tommie and You? Be succinct.\n",
      "\n",
      "### Input:\n",
      "Context from memory:\n",
      "- April 23, 2023, 09:51 PM: Tommie tries to get some rest.\n",
      "- April 23, 2023, 09:50 PM: Tommie remembers his dog, Bruno, from when he was a kid\n",
      "- April 23, 2023, 09:51 PM: Tommie is hungry\n",
      "- April 23, 2023, 09:51 PM: The road is noisy at night\n",
      "- April 23, 2023, 09:51 PM: The new neighbors have a cat\n",
      "- April 23, 2\n",
      "Most recent observations: Tommie remembers his dog, Bruno, from when he was a kid; Tommie feels tired from driving so far; Tommie sees the new home; The new neighbors have a cat; The road is noisy at night; Tommie is hungry; Tommie tries to get some rest.; Tommie observed Beau says What do you like to do? and said \"what to say\".\n",
      "Observation: Beau says What are you most worried about today?\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     6.65 ms /    19 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time = 11904.61 ms /   570 tokens (   20.89 ms per token)\n",
      "llama_print_timings:        eval time =  7604.92 ms /    18 runs   (  422.50 ms per run)\n",
      "llama_print_timings:       total time = 19517.53 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You are listening to Tommie talk about himself in the first person perspective.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interview_agent(tommie, \"What are you most worried about today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have Tommie start going through a day in the life.\n",
    "observations = [\n",
    "    \"Tommie wakes up to the sound of a noisy construction site outside his window.\",\n",
    "    \"Tommie gets out of bed and heads to the kitchen to make himself some coffee.\",\n",
    "    \"Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some.\",\n",
    "    \"Tommie finally finds the filters and makes himself a cup of coffee.\",\n",
    "    \"The coffee tastes bitter, and Tommie regrets not buying a better brand.\",\n",
    "    \"Tommie checks his email and sees that he has no job offers yet.\",\n",
    "    \"Tommie spends some time updating his resume and cover letter.\",\n",
    "    \"Tommie heads out to explore the city and look for job openings.\",\n",
    "    \"Tommie sees a sign for a job fair and decides to attend.\",\n",
    "    \"The line to get in is long, and Tommie has to wait for an hour.\",\n",
    "    \"Tommie meets several potential employers at the job fair but doesn't receive any offers.\",\n",
    "    \"Tommie leaves the job fair feeling disappointed.\",\n",
    "    \"Tommie stops by a local diner to grab some lunch.\",\n",
    "    \"The service is slow, and Tommie has to wait for 30 minutes to get his food.\",\n",
    "    \"Tommie overhears a conversation at the next table about a job opening.\",\n",
    "    \"Tommie asks the diners about the job opening and gets some information about the company.\",\n",
    "    \"Tommie decides to apply for the job and sends his resume and cover letter.\",\n",
    "    \"Tommie continues his search for job openings and drops off his resume at several local businesses.\",\n",
    "    \"Tommie takes a break from his job search to go for a walk in a nearby park.\",\n",
    "    \"A dog approaches and licks Tommie's feet, and he pets it for a few minutes.\",\n",
    "    \"Tommie sees a group of people playing frisbee and decides to join in.\",\n",
    "    \"Tommie has fun playing frisbee but gets hit in the face with the frisbee and hurts his nose.\",\n",
    "    \"Tommie goes back to his apartment to rest for a bit.\",\n",
    "    \"A raccoon tore open the trash bag outside his apartment, and the garbage is all over the floor.\",\n",
    "    \"Tommie starts to feel frustrated with his job search.\",\n",
    "    \"Tommie calls his best friend to vent about his struggles.\",\n",
    "    \"Tommie's friend offers some words of encouragement and tells him to keep trying.\",\n",
    "    \"Tommie feels slightly better after talking to his friend.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog is chasing a ball.\n",
      "\n",
      "### Response:\n",
      "The dog\n",
      "\n",
      "### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "Tommie wakes up to the sound of a noisy construction site outside his window.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What is The dog doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog jumps into the air and catches the ball.\n",
      "\n",
      "### Response:\n",
      "Catching a ball, possibly playing fetch\n",
      "\n",
      "### Instruction:\n",
      "What is Tommie doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "Tommie wakes up to the sound of a noisy construction site outside his window.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     3.55 ms /    10 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time =  2747.16 ms /    93 tokens (   29.54 ms per token)\n",
      "llama_print_timings:        eval time =  3416.70 ms /     9 runs   (  379.63 ms per run)\n",
      "llama_print_timings:       total time =  6168.20 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     6.73 ms /    19 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time =  2671.67 ms /   104 tokens (   25.69 ms per token)\n",
      "llama_print_timings:        eval time =  6839.22 ms /    18 runs   (  379.96 ms per run)\n",
      "llama_print_timings:       total time =  9519.08 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2403.55 ms /    30 tokens (   80.12 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2404.20 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  1303.66 ms /    16 tokens (   81.48 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  1303.83 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Given the provided information, what is the relationship between Tommie and Tommie? Be succinct.\n",
      "\n",
      "### Input:\n",
      "Context from memory:\n",
      "- April 23, 2023, 09:51 PM: Tommie tries to get some rest.\n",
      "- April 23, 2023, 09:50 PM: Tommie remembers his dog, Bruno, from when he was a kid\n",
      "- April 23, 2023, 09:51 PM: Tommie is hungry\n",
      "- April 23, 2023, 09:51 PM: The road is noisy at night\n",
      "- April 23, 2023, 09:51 PM: The new neighbors have a cat\n",
      "- April 23, 2023, 09:50 PM: Tommie feels tired from driving so far\n",
      "- April 23, 2023, 09:53 PM: Tommie observed Beau says What do you like to do? and said \"what to say\".\n",
      "- April 23, 2023, 09:50 PM: Tommie sees the new home\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =    58.36 ms /   162 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time =  5051.81 ms /   296 tokens (   17.07 ms per token)\n",
      "llama_print_timings:        eval time = 65503.83 ms /   161 runs   (  406.86 ms per run)\n",
      "llama_print_timings:       total time = 70630.11 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Should Tommie react to the observation, and if so, what would be an appropriate reaction? Respond in one line. If the action is to engage in dialogue, write:\n",
      "SAY: \"what to say\"\n",
      "otherwise, write:\n",
      "REACT: Tommie's reaction (if anything).\n",
      "Either do nothing, react, or say something but not both.\n",
      "\n",
      "### Input:\n",
      "Name: Tommie (age: 25)\n",
      "Innate traits: anxious, likes design\n",
      "Tommie is an older man who has fond memories of his childhood pet, Bruno. He is a homebody and enjoys the comforts of home. Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "It is April 23, 2023, 09:59 PM.\n",
      "Tommie's status: looking for a job\n",
      "Summary of relevant context from Tommie's memory:\n",
      "Tommie is a person. Tommie is a dog. Tommie is hungry. The road is noisy at night. The new neighbors have a cat. Tommie feels tired from driving so far. Tommie observed Beau says What do you like to do? and said \"what to say\". Tommie sees the new home.\n",
      "```\n",
      "\n",
      "### Output:\n",
      "Tommie is a person, Tommie is a dog, Tommie is hungry, The road is noisy at night, The new neighbors have a cat, Tommie feels tired from driving so far, Tommie observed Beau says What do you like to do? and said \"what to say\", Tommie sees the new home.\n",
      "```\n",
      "Most recent observations: Tommie remembers his dog, Bruno, from when he was a kid; Tommie feels tired from driving so far; Tommie sees the new home; The new neighbors have a cat; The road is noisy at night; Tommie is hungry; Tommie tries to get some rest.; Tommie observed Beau says What do you like to do? and said \"what to say\".\n",
      "Observation: Tommie wakes up to the sound of a noisy construction site outside his window.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie remembers brushing his teeth\n",
      "\n",
      "### Response:\n",
      "Rating: 1\n",
      "\n",
      "### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie observed Tommie wakes up to the sound of a noisy construction site outside his window. and reacted by Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "\n",
      "### Response:\n",
      "Rating:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =    90.41 ms /   256 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time = 10074.98 ms /   518 tokens (   19.45 ms per token)\n",
      "llama_print_timings:        eval time = 110495.17 ms /   255 runs   (  433.31 ms per run)\n",
      "llama_print_timings:       total time = 120688.18 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "0.015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     3.10 ms /     9 runs   (    0.34 ms per run)\n",
      "llama_print_timings: prompt eval time =  4461.89 ms /   266 tokens (   16.77 ms per token)\n",
      "llama_print_timings:        eval time =  3163.79 ms /     8 runs   (  395.47 ms per run)\n",
      "llama_print_timings:       total time =  7629.40 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2316.29 ms /    58 tokens (   39.94 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2316.65 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tommie wakes up to the sound of a noisy construction site outside his window. Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog is chasing a ball.\n",
      "\n",
      "### Response:\n",
      "The dog\n",
      "\n",
      "### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "Tommie gets out of bed and heads to the kitchen to make himself some coffee.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What is The dog doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog jumps into the air and catches the ball.\n",
      "\n",
      "### Response:\n",
      "Catching a ball, possibly playing fetch\n",
      "\n",
      "### Instruction:\n",
      "What is Tommie doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "Tommie gets out of bed and heads to the kitchen to make himself some coffee.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     3.51 ms /    10 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time =  2700.24 ms /    92 tokens (   29.35 ms per token)\n",
      "llama_print_timings:        eval time =  3400.49 ms /     9 runs   (  377.83 ms per run)\n",
      "llama_print_timings:       total time =  6104.90 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     5.62 ms /    16 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time =  2654.36 ms /   103 tokens (   25.77 ms per token)\n",
      "llama_print_timings:        eval time =  5682.32 ms /    15 runs   (  378.82 ms per run)\n",
      "llama_print_timings:       total time =  8343.38 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2395.70 ms /    30 tokens (   79.86 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2396.10 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  1146.28 ms /    14 tokens (   81.88 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  1146.48 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Given the provided information, what is the relationship between Tommie and Tommie? Be succinct.\n",
      "\n",
      "### Input:\n",
      "Context from memory:\n",
      "- April 23, 2023, 09:51 PM: Tommie tries to get some rest.\n",
      "- April 23, 2023, 09:50 PM: Tommie remembers his dog, Bruno, from when he was a kid\n",
      "- April 23, 2023, 10:01 PM: Tommie observed Tommie wakes up to the sound of a noisy construction site outside his window. and reacted by Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "- April 23, 2023, 09:51 PM: Tommie is hungry\n",
      "- April 23, 2023, 09:51 PM: The road is noisy at night\n",
      "- April 23, 2023, 09:51 PM: The new neighbors have a cat\n",
      "- April 23, 2023, 09:50 PM: Tommie feels tired from driving so far\n",
      "- April 23, 2023, 09:53 PM: Tommie observed Beau says What do you like to do? and said \"what to say\".\n",
      "- April 23, 2023, 09:50 PM: Tommie sees the new home\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =    65.78 ms /   184 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time =  5996.11 ms /   374 tokens (   16.03 ms per token)\n",
      "llama_print_timings:        eval time = 75996.89 ms /   183 runs   (  415.28 ms per run)\n",
      "llama_print_timings:       total time = 82078.79 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Should Tommie react to the observation, and if so, what would be an appropriate reaction? Respond in one line. If the action is to engage in dialogue, write:\n",
      "SAY: \"what to say\"\n",
      "otherwise, write:\n",
      "REACT: Tommie's reaction (if anything).\n",
      "Either do nothing, react, or say something but not both.\n",
      "\n",
      "### Input:\n",
      "Name: Tommie (age: 25)\n",
      "Innate traits: anxious, likes design\n",
      "Tommie is an older man who has fond memories of his childhood pet, Bruno. He is a homebody and enjoys the comforts of home. Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "It is April 23, 2023, 10:02 PM.\n",
      "Tommie's status: looking for a job\n",
      "Summary of relevant context from Tommie's memory:\n",
      "Tommie is a man who has been driving for long distances. He feels tired from driving so far and needs rest. The road is noisy at night which makes it difficult to sleep. Tommie observes that he is hungry, and then remembers his dog Bruno when he was a kid. Later on, Tommie observed Beau says What do you like to do? and said \"what to say\".\n",
      "```\n",
      "\n",
      "### Output:\n",
      "Tommie is a man who has been driving for long distances. He feels tired from driving so far and needs rest. The road is noisy at night which makes it difficult to sleep. Tommie observes that he is hungry, and then remembers his dog Bruno when he was a kid. Later on, Tommie observed Beau says What do you like to do? and said \"what to say\".\n",
      "```\n",
      "Most recent observations: Tommie remembers his dog, Bruno, from when he was a kid; Tommie feels tired from driving so far; Tommie sees the new home; The new neighbors have a cat; The road is noisy at night; Tommie is hungry; Tommie tries to get some rest.; Tommie observed Beau says What do you like to do? and said \"what to say\".; Tommie observed Tommie wakes up to the sound of a noisy construction site outside his window. and reacted by Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "Observation: Tommie gets out of bed and heads to the kitchen to make himself some coffee.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie remembers brushing his teeth\n",
      "\n",
      "### Response:\n",
      "Rating: 1\n",
      "\n",
      "### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie observed Tommie gets out of bed and heads to the kitchen to make himself some coffee. and reacted by Tommie wakes up to the sound of a noisy construction site outside his window.\n",
      "\n",
      "### Response:\n",
      "Rating:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     9.92 ms /    28 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time = 12404.14 ms /   597 tokens (   20.78 ms per token)\n",
      "llama_print_timings:        eval time = 11592.92 ms /    27 runs   (  429.37 ms per run)\n",
      "llama_print_timings:       total time = 24008.94 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "0.015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     3.20 ms /     9 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time =  4180.96 ms /   254 tokens (   16.46 ms per token)\n",
      "llama_print_timings:        eval time =  3149.82 ms /     8 runs   (  393.73 ms per run)\n",
      "llama_print_timings:       total time =  7334.61 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2253.78 ms /    46 tokens (   49.00 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2254.06 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tommie gets out of bed and heads to the kitchen to make himself some coffee. Tommie wakes up to the sound of a noisy construction site outside his window.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog is chasing a ball.\n",
      "\n",
      "### Response:\n",
      "The dog\n",
      "\n",
      "### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What is The dog doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog jumps into the air and catches the ball.\n",
      "\n",
      "### Response:\n",
      "Catching a ball, possibly playing fetch\n",
      "\n",
      "### Instruction:\n",
      "What is Tommie doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     3.56 ms /    10 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time =  2596.95 ms /    98 tokens (   26.50 ms per token)\n",
      "llama_print_timings:        eval time =  3424.97 ms /     9 runs   (  380.55 ms per run)\n",
      "llama_print_timings:       total time =  6026.30 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     6.01 ms /    17 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time =  2713.43 ms /   109 tokens (   24.89 ms per token)\n",
      "llama_print_timings:        eval time =  6096.54 ms /    16 runs   (  381.03 ms per run)\n",
      "llama_print_timings:       total time =  8817.21 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2407.11 ms /    30 tokens (   80.24 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2407.42 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  1221.42 ms /    15 tokens (   81.43 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  1221.61 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Given the provided information, what is the relationship between Tommie and Tommie? Be succinct.\n",
      "\n",
      "### Input:\n",
      "Context from memory:\n",
      "- April 23, 2023, 09:51 PM: Tommie tries to get some rest.\n",
      "- April 23, 2023, 09:50 PM: Tommie remembers his dog, Bruno, from when he was a kid\n",
      "- April 23, 2023, 10:01 PM: Tommie observed Tommie wakes up to the sound of a noisy construction site outside his window. and reacted by Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "- April 23, 2023, 09:51 PM: Tommie is hungry\n",
      "- April 23, 2023, 09:51 PM: The road is noisy at night\n",
      "- April 23, 2023, 09:51 PM: The new neighbors have a cat\n",
      "- April 23, 2023, 10:03 PM: Tommie observed Tommie gets out of bed and heads to the kitchen to make himself some coffee. and reacted by Tommie wakes up to the sound of a noisy construction site outside his window.\n",
      "- April 23, 2023, 09:50 PM: Tommie feels tired from driving so far\n",
      "- April 23, 2023, 09:53 PM: Tommie observed Beau says What do you like to do? and said \"what to say\".\n",
      "- April 23, 2023, 09:50 PM: Tommie sees the new home\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =    80.98 ms /   228 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time =  7232.34 ms /   440 tokens (   16.44 ms per token)\n",
      "llama_print_timings:        eval time = 96278.45 ms /   227 runs   (  424.13 ms per run)\n",
      "llama_print_timings:       total time = 103618.04 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Should Tommie react to the observation, and if so, what would be an appropriate reaction? Respond in one line. If the action is to engage in dialogue, write:\n",
      "SAY: \"what to say\"\n",
      "otherwise, write:\n",
      "REACT: Tommie's reaction (if anything).\n",
      "Either do nothing, react, or say something but not both.\n",
      "\n",
      "### Input:\n",
      "Name: Tommie (age: 25)\n",
      "Innate traits: anxious, likes design\n",
      "Tommie is an older man who has fond memories of his childhood pet, Bruno. He is a homebody and enjoys the comforts of home. Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "It is April 23, 2023, 10:05 PM.\n",
      "Tommie's status: looking for a job\n",
      "Summary of relevant context from Tommie's memory:\n",
      "Tommie is a man who lives in a house. He has a dog named Bruno, and he feels tired from driving long distances. The road is noisy at night which makes it difficult to sleep. Tommie observes that he is hungry and wakes up to the sound of a noisy construction site outside his window. He reacts by getting out of bed and heading to the kitchen to make himself some coffee. Beau asks him what he likes to do, and Tommie says \"what to say\".\n",
      "```\n",
      "\n",
      "### Output:\n",
      "Tommie is a man who lives in a house. He has a dog named Bruno, and he feels tired from driving long distances. The road is noisy at night which makes it difficult to sleep. Tommie observes that he is hungry and wakes up to the sound of a noisy construction site outside his window. He reacts by getting out of bed and heading to the kitchen to make himself some coffee. Beau asks him what he likes to do, and Tommie says \"what to say\".\n",
      "```\n",
      "Most recent observations: Tommie remembers his dog, Bruno, from when he was a kid; Tommie feels tired from driving so far; Tommie sees the new home; The new neighbors have a cat; The road is noisy at night; Tommie is hungry; Tommie tries to get some rest.; Tommie observed Beau says What do you like to do? and said \"what to say\".; Tommie observed Tommie wakes up to the sound of a noisy construction site outside his window. and reacted by Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.; Tommie observed Tommie gets out of bed and heads to the kitchen to make himself some coffee. and reacted by Tommie wakes up to the sound of a noisy construction site outside his window.\n",
      "Observation: Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie remembers brushing his teeth\n",
      "\n",
      "### Response:\n",
      "Rating: 1\n",
      "\n",
      "### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie observed Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some. and reacted by ```\n",
      "\n",
      "### Response:\n",
      "Rating:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =    25.24 ms /    71 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time = 13490.88 ms /   692 tokens (   19.50 ms per token)\n",
      "llama_print_timings:        eval time = 30917.45 ms /    70 runs   (  441.68 ms per run)\n",
      "llama_print_timings:       total time = 44439.44 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "0.015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     1.71 ms /     5 runs   (    0.34 ms per run)\n",
      "llama_print_timings: prompt eval time =  4057.76 ms /   242 tokens (   16.77 ms per token)\n",
      "llama_print_timings:        eval time =  1584.12 ms /     4 runs   (  396.03 ms per run)\n",
      "llama_print_timings:       total time =  5644.03 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2179.33 ms /    34 tokens (   64.10 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2179.67 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some. ```\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog is chasing a ball.\n",
      "\n",
      "### Response:\n",
      "The dog\n",
      "\n",
      "### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "Tommie finally finds the filters and makes himself a cup of coffee.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     3.60 ms /    10 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time =  2688.18 ms /    89 tokens (   30.20 ms per token)\n",
      "llama_print_timings:        eval time =  3397.43 ms /     9 runs   (  377.49 ms per run)\n",
      "llama_print_timings:       total time =  6089.96 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What is The dog doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog jumps into the air and catches the ball.\n",
      "\n",
      "### Response:\n",
      "Catching a ball, possibly playing fetch\n",
      "\n",
      "### Instruction:\n",
      "What is Tommie doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "Tommie finally finds the filters and makes himself a cup of coffee.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     3.52 ms /    10 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time =  2630.36 ms /   100 tokens (   26.30 ms per token)\n",
      "llama_print_timings:        eval time =  3403.32 ms /     9 runs   (  378.15 ms per run)\n",
      "llama_print_timings:       total time =  6037.89 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2396.37 ms /    30 tokens (   79.88 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2396.71 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =   692.44 ms /     8 tokens (   86.56 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =   692.59 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Given the provided information, what is the relationship between Tommie and Tommie? Be succinct.\n",
      "\n",
      "### Input:\n",
      "Context from memory:\n",
      "- April 23, 2023, 09:51 PM: Tommie tries to get some rest.\n",
      "- April 23, 2023, 09:50 PM: Tommie remembers his dog, Bruno, from when he was a kid\n",
      "- April 23, 2023, 10:01 PM: Tommie observed Tommie wakes up to the sound of a noisy construction site outside his window. and reacted by Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "- April 23, 2023, 09:51 PM: Tommie is hungry\n",
      "- April 23, 2023, 09:51 PM: The road is noisy at night\n",
      "- April 23, 2023, 09:51 PM: The new neighbors have a cat\n",
      "- April 23, 2023, 10:03 PM: Tommie observed Tommie gets out of bed and heads to the kitchen to make himself some coffee. and reacted by Tommie wakes up to the sound of a noisy construction site outside his window.\n",
      "- April 23, 2023, 09:50 PM: Tommie feels tired from driving so far\n",
      "- April 23, 2023, 10:06 PM: Tommie observed Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some. and reacted by ```\n",
      "- April 23, 2023, 09:53 PM: Tommie observed Beau says What do you like to do? and said \"what to say\".\n",
      "- April 23, 2023, 09:50 PM: Tommie sees the new home\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =    54.66 ms /   154 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time =  8539.96 ms /   494 tokens (   17.29 ms per token)\n",
      "llama_print_timings:        eval time = 65094.59 ms /   153 runs   (  425.45 ms per run)\n",
      "llama_print_timings:       total time = 73704.43 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Should Tommie react to the observation, and if so, what would be an appropriate reaction? Respond in one line. If the action is to engage in dialogue, write:\n",
      "SAY: \"what to say\"\n",
      "otherwise, write:\n",
      "REACT: Tommie's reaction (if anything).\n",
      "Either do nothing, react, or say something but not both.\n",
      "\n",
      "### Input:\n",
      "Name: Tommie (age: 25)\n",
      "Innate traits: anxious, likes design\n",
      "Tommie is an older man who has fond memories of his childhood pet, Bruno. He is a homebody and enjoys the comforts of home. Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "It is April 23, 2023, 10:07 PM.\n",
      "Tommie's status: looking for a job\n",
      "Summary of relevant context from Tommie's memory:\n",
      "Tommie is a person. Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep. Tommie is hungry. The new neighbors have a cat. Tommie gets out of bed and heads to the kitchen to make himself some coffee. Tommie wakes up to the sound of a noisy construction site outside his window. Tommie feels tired from driving so far. Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some. Beau says What do you like to do? and said \"what to say\". Tommie observed Tommie sees the new home.\n",
      "```\n",
      "Most recent observations: Tommie remembers his dog, Bruno, from when he was a kid; Tommie feels tired from driving so far; Tommie sees the new home; The new neighbors have a cat; The road is noisy at night; Tommie is hungry; Tommie tries to get some rest.; Tommie observed Beau says What do you like to do? and said \"what to say\".; Tommie observed Tommie wakes up to the sound of a noisy construction site outside his window. and reacted by Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.; Tommie observed Tommie gets out of bed and heads to the kitchen to make himself some coffee. and reacted by Tommie wakes up to the sound of a noisy construction site outside his window.; Tommie observed Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some. and reacted by ```\n",
      "Observation: Tommie finally finds the filters and makes himself a cup of coffee.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie remembers brushing his teeth\n",
      "\n",
      "### Response:\n",
      "Rating: 1\n",
      "\n",
      "### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie observed Tommie finally finds the filters and makes himself a cup of coffee. and reacted by ```\n",
      "\n",
      "### Response:\n",
      "Rating:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     6.49 ms /    18 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time = 13007.57 ms /   642 tokens (   20.26 ms per token)\n",
      "llama_print_timings:        eval time =  7373.56 ms /    17 runs   (  433.74 ms per run)\n",
      "llama_print_timings:       total time = 20389.10 ms\n",
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     1.78 ms /     5 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time =  3928.51 ms /   233 tokens (   16.86 ms per token)\n",
      "llama_print_timings:        eval time =  1568.32 ms /     4 runs   (  392.08 ms per run)\n",
      "llama_print_timings:       total time =  5499.00 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "0.015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2011.74 ms /    25 tokens (   80.47 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2012.03 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tommie finally finds the filters and makes himself a cup of coffee. ```\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog is chasing a ball.\n",
      "\n",
      "### Response:\n",
      "The dog\n",
      "\n",
      "### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The coffee tastes bitter, and Tommie regrets not buying a better brand.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What is The dog doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog jumps into the air and catches the ball.\n",
      "\n",
      "### Response:\n",
      "Catching a ball, possibly playing fetch\n",
      "\n",
      "### Instruction:\n",
      "What is Tommie doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The coffee tastes bitter, and Tommie regrets not buying a better brand.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     3.49 ms /    10 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time =  2714.97 ms /    94 tokens (   28.88 ms per token)\n",
      "llama_print_timings:        eval time =  3404.64 ms /     9 runs   (  378.29 ms per run)\n",
      "llama_print_timings:       total time =  6123.76 ms\n",
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     8.61 ms /    24 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time =  2672.93 ms /   105 tokens (   25.46 ms per token)\n",
      "llama_print_timings:        eval time =  8725.45 ms /    23 runs   (  379.37 ms per run)\n",
      "llama_print_timings:       total time = 11408.60 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2413.20 ms /    30 tokens (   80.44 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2413.57 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  1772.74 ms /    22 tokens (   80.58 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  1772.92 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Given the provided information, what is the relationship between Tommie and Tommie? Be succinct.\n",
      "\n",
      "### Input:\n",
      "Context from memory:\n",
      "- April 23, 2023, 09:51 PM: Tommie tries to get some rest.\n",
      "- April 23, 2023, 09:50 PM: Tommie remembers his dog, Bruno, from when he was a kid\n",
      "- April 23, 2023, 10:01 PM: Tommie observed Tommie wakes up to the sound of a noisy construction site outside his window. and reacted by Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "- April 23, 2023, 09:51 PM: Tommie is hungry\n",
      "- April 23, 2023, 09:51 PM: The road is noisy at night\n",
      "- April 23, 2023, 09:51 PM: The new neighbors have a cat\n",
      "- April 23, 2023, 10:03 PM: Tommie observed Tommie gets out of bed and heads to the kitchen to make himself some coffee. and reacted by Tommie wakes up to the sound of a noisy construction site outside his window.\n",
      "- April 23, 2023, 09:50 PM: Tommie feels tired from driving so far\n",
      "- April 23, 2023, 10:06 PM: Tommie observed Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some. and reacted by ```\n",
      "- April 23, 2023, 10:08 PM: Tommie observed Tommie finally finds the filters and makes himself a cup of coffee. and reacted by ```\n",
      "- April 23, 2023, 09:53 PM: Tommie observed Beau says What do you like to do? and said \"what to say\".\n",
      "- April 23, 2023, 09:50 PM: Tommie sees the new home\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =    13.72 ms /    38 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time = 12832.17 ms /   539 tokens (   23.81 ms per token)\n",
      "llama_print_timings:        eval time = 15652.22 ms /    37 runs   (  423.03 ms per run)\n",
      "llama_print_timings:       total time = 28500.87 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Should Tommie react to the observation, and if so, what would be an appropriate reaction? Respond in one line. If the action is to engage in dialogue, write:\n",
      "SAY: \"what to say\"\n",
      "otherwise, write:\n",
      "REACT: Tommie's reaction (if anything).\n",
      "Either do nothing, react, or say something but not both.\n",
      "\n",
      "### Input:\n",
      "Name: Tommie (age: 25)\n",
      "Innate traits: anxious, likes design\n",
      "Tommie is an older man who has fond memories of his childhood pet, Bruno. He is a homebody and enjoys the comforts of home. Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "It is April 23, 2023, 10:09 PM.\n",
      "Tommie's status: looking for a job\n",
      "Summary of relevant context from Tommie's memory:\n",
      "Tommie is a person.\n",
      "```\n",
      "\n",
      "### Output:\n",
      "```\n",
      "What is the relationship between Tommie and Tommie? Be succinct.\n",
      "```\n",
      "Most recent observations: Tommie remembers his dog, Bruno, from when he was a kid; Tommie feels tired from driving so far; Tommie sees the new home; The new neighbors have a cat; The road is noisy at night; Tommie is hungry; Tommie tries to get some rest.; Tommie observed Beau says What do you like to do? and said \"what to say\".; Tommie observed Tommie wakes up to the sound of a noisy construction site outside his window. and reacted by Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.; Tommie observed Tommie gets out of bed and heads to the kitchen to make himself some coffee. and reacted by Tommie wakes up to the sound of a noisy construction site outside his window.; Tommie observed Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some. and reacted by ```; Tommie observed Tommie finally finds the filters and makes himself a cup of coffee. and reacted by ```\n",
      "Observation: The coffee tastes bitter, and Tommie regrets not buying a better brand.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie remembers brushing his teeth\n",
      "\n",
      "### Response:\n",
      "Rating: 1\n",
      "\n",
      "### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie observed The coffee tastes bitter, and Tommie regrets not buying a better brand. and reacted by ```\n",
      "\n",
      "### Response:\n",
      "Rating:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     9.51 ms /    27 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time = 11833.62 ms /   556 tokens (   21.28 ms per token)\n",
      "llama_print_timings:        eval time = 11050.98 ms /    26 runs   (  425.04 ms per run)\n",
      "llama_print_timings:       total time = 22896.12 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "0.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     1.69 ms /     5 runs   (    0.34 ms per run)\n",
      "llama_print_timings: prompt eval time =  4055.86 ms /   238 tokens (   17.04 ms per token)\n",
      "llama_print_timings:        eval time =  1574.05 ms /     4 runs   (  393.51 ms per run)\n",
      "llama_print_timings:       total time =  5632.00 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coffee tastes bitter, and Tommie regrets not buying a better brand. ```\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog is chasing a ball.\n",
      "\n",
      "### Response:\n",
      "The dog\n",
      "\n",
      "### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "Tommie checks his email and sees that he has no job offers yet.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2423.81 ms /    30 tokens (   80.79 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2424.02 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What is The dog doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog jumps into the air and catches the ball.\n",
      "\n",
      "### Response:\n",
      "Catching a ball, possibly playing fetch\n",
      "\n",
      "### Instruction:\n",
      "What is Tommie doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "Tommie checks his email and sees that he has no job offers yet.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     3.52 ms /    10 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time =  2704.19 ms /    90 tokens (   30.05 ms per token)\n",
      "llama_print_timings:        eval time =  3397.24 ms /     9 runs   (  377.47 ms per run)\n",
      "llama_print_timings:       total time =  6105.65 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     7.31 ms /    21 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time =  2650.62 ms /   101 tokens (   26.24 ms per token)\n",
      "llama_print_timings:        eval time =  7583.81 ms /    20 runs   (  379.19 ms per run)\n",
      "llama_print_timings:       total time = 10243.08 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2400.03 ms /    30 tokens (   80.00 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2400.32 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  1539.20 ms /    19 tokens (   81.01 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  1539.37 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Given the provided information, what is the relationship between Tommie and Tommie? Be succinct.\n",
      "\n",
      "### Input:\n",
      "Context from memory:\n",
      "- April 23, 2023, 09:51 PM: Tommie tries to get some rest.\n",
      "- April 23, 2023, 09:50 PM: Tommie remembers his dog, Bruno, from when he was a kid\n",
      "- April 23, 2023, 10:01 PM: Tommie observed Tommie wakes up to the sound of a noisy construction site outside his window. and reacted by Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "- April 23, 2023, 09:51 PM: Tommie is hungry\n",
      "- April 23, 2023, 09:51 PM: The road is noisy at night\n",
      "- April 23, 2023, 09:51 PM: The new neighbors have a cat\n",
      "- April 23, 2023, 10:03 PM: Tommie observed Tommie gets out of bed and heads to the kitchen to make himself some coffee. and reacted by Tommie wakes up to the sound of a noisy construction site outside his window.\n",
      "- April 23, 2023, 09:50 PM: Tommie feels tired from driving so far\n",
      "- April 23, 2023, 10:06 PM: Tommie observed Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some. and reacted by ```\n",
      "- April 23, 2023, 10:09 PM: Tommie observed The coffee tastes bitter, and Tommie regrets not buying a better brand. and reacted by ```\n",
      "- April 23, 2023, 10:08 PM: Tommie observed Tommie finally finds the filters and makes himself a cup of coffee. and reacted by ```\n",
      "- April 23, 2023, 09:53 PM: Tommie observed Beau says What do you like to do? and said \"what to say\".\n",
      "- April 23, 2023, 09:50 PM: Tommie sees the new home\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =    72.40 ms /   205 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time = 12392.25 ms /   589 tokens (   21.04 ms per token)\n",
      "llama_print_timings:        eval time = 89451.88 ms /   204 runs   (  438.49 ms per run)\n",
      "llama_print_timings:       total time = 101938.49 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Should Tommie react to the observation, and if so, what would be an appropriate reaction? Respond in one line. If the action is to engage in dialogue, write:\n",
      "SAY: \"what to say\"\n",
      "otherwise, write:\n",
      "REACT: Tommie's reaction (if anything).\n",
      "Either do nothing, react, or say something but not both.\n",
      "\n",
      "### Input:\n",
      "Name: Tommie (age: 25)\n",
      "Innate traits: anxious, likes design\n",
      "Tommie is an older man who has fond memories of his childhood pet, Bruno. He is a homebody and enjoys the comforts of home. Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "It is April 23, 2023, 10:11 PM.\n",
      "Tommie's status: looking for a job\n",
      "Summary of relevant context from Tommie's memory:\n",
      "Tommie is a person. Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep. Tommie is hungry. The new neighbors have a cat. Tommie gets out of bed and heads to the kitchen to make himself some coffee. Tommie wakes up to the sound of a noisy construction site outside his window. Tommie feels tired from driving so far. Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some. The coffee tastes bitter, and Tommie regrets not buying a better brand. Tommie finally finds the filters and makes himself a cup of coffee. Beau says What do you like to do? and said \"what to say\". Tommie observed Beau says What do you like to do? and said \"what to say\". Tommie sees the new home.\n",
      "```\n",
      "Most recent observations: Tommie remembers his dog, Bruno, from when he was a kid; Tommie feels tired from driving so far; Tommie sees the new home; The new neighbors have a cat; The road is noisy at night; Tommie is hungry; Tommie tries to get some rest.; Tommie observed Beau says What do you like to do? and said \"what to say\".; Tommie observed Tommie wakes up to the sound of a noisy construction site outside his window. and reacted by Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.; Tommie observed Tommie gets out of bed and heads to the kitchen to make himself some coffee. and reacted by Tommie wakes up to the sound of a noisy construction site outside his window.; Tommie observed Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some. and reacted by ```; Tommie observed Tommie finally finds the filters and makes himself a cup of coffee. and reacted by ```; Tommie observed The coffee tastes bitter, and Tommie regrets not buying a better brand. and reacted by ```\n",
      "Observation: Tommie checks his email and sees that he has no job offers yet.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     6.49 ms /    18 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time = 14483.72 ms /   749 tokens (   19.34 ms per token)\n",
      "llama_print_timings:        eval time =  7524.27 ms /    17 runs   (  442.60 ms per run)\n",
      "llama_print_timings:       total time = 22015.79 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie remembers brushing his teeth\n",
      "\n",
      "### Response:\n",
      "Rating: 1\n",
      "\n",
      "### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie observed Tommie checks his email and sees that he has no job offers yet. and reacted by ```\n",
      "\n",
      "### Response:\n",
      "Rating:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "0.015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     1.78 ms /     5 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time =  4003.52 ms /   234 tokens (   17.11 ms per token)\n",
      "llama_print_timings:        eval time =  1565.33 ms /     4 runs   (  391.33 ms per run)\n",
      "llama_print_timings:       total time =  5571.07 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2091.46 ms /    26 tokens (   80.44 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2091.68 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tommie checks his email and sees that he has no job offers yet. ```\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog is chasing a ball.\n",
      "\n",
      "### Response:\n",
      "The dog\n",
      "\n",
      "### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "Tommie spends some time updating his resume and cover letter.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What is The dog doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog jumps into the air and catches the ball.\n",
      "\n",
      "### Response:\n",
      "Catching a ball, possibly playing fetch\n",
      "\n",
      "### Instruction:\n",
      "What is Tommie doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "Tommie spends some time updating his resume and cover letter.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     3.53 ms /    10 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time =  2714.11 ms /    89 tokens (   30.50 ms per token)\n",
      "llama_print_timings:        eval time =  3405.60 ms /     9 runs   (  378.40 ms per run)\n",
      "llama_print_timings:       total time =  6123.89 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     6.06 ms /    17 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time =  2633.45 ms /   100 tokens (   26.33 ms per token)\n",
      "llama_print_timings:        eval time =  6055.12 ms /    16 runs   (  378.45 ms per run)\n",
      "llama_print_timings:       total time =  8695.79 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2432.44 ms /    30 tokens (   81.08 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2432.87 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  1222.83 ms /    15 tokens (   81.52 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  1223.00 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Given the provided information, what is the relationship between Tommie and Tommie? Be succinct.\n",
      "\n",
      "### Input:\n",
      "Context from memory:\n",
      "- April 23, 2023, 09:51 PM: Tommie tries to get some rest.\n",
      "- April 23, 2023, 09:50 PM: Tommie remembers his dog, Bruno, from when he was a kid\n",
      "- April 23, 2023, 10:01 PM: Tommie observed Tommie wakes up to the sound of a noisy construction site outside his window. and reacted by Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "- April 23, 2023, 09:51 PM: Tommie is hungry\n",
      "- April 23, 2023, 09:51 PM: The road is noisy at night\n",
      "- April 23, 2023, 09:51 PM: The new neighbors have a cat\n",
      "- April 23, 2023, 10:03 PM: Tommie observed Tommie gets out of bed and heads to the kitchen to make himself some coffee. and reacted by Tommie wakes up to the sound of a noisy construction site outside his window.\n",
      "- April 23, 2023, 09:50 PM: Tommie feels tired from driving so far\n",
      "- April 23, 2023, 10:06 PM: Tommie observed Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some. and reacted by ```\n",
      "- April 23, 2023, 10:12 PM: Tommie observed Tommie checks his email and sees that he has no job offers yet. and reacted by ```\n",
      "- April 23, 2023, 10:09 PM: Tommie observed The coffee tastes bitter, and Tommie regrets not buying a better brand. and reacted by ```\n",
      "- April 23, 2023, 10:08 PM: Tommie observed Tommie finally finds the filters and makes himself a cup of coffee. and reacted by ```\n",
      "- April 23, 2023, 09:53 PM: Tommie observed Beau says What do you like to do? and said \"what to say\".\n",
      "- April 23, 2023, 09:50 PM: Tommie sees the new home\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =    12.21 ms /    34 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time = 12745.95 ms /   635 tokens (   20.07 ms per token)\n",
      "llama_print_timings:        eval time = 14471.74 ms /    33 runs   (  438.54 ms per run)\n",
      "llama_print_timings:       total time = 27232.18 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Should Tommie react to the observation, and if so, what would be an appropriate reaction? Respond in one line. If the action is to engage in dialogue, write:\n",
      "SAY: \"what to say\"\n",
      "otherwise, write:\n",
      "REACT: Tommie's reaction (if anything).\n",
      "Either do nothing, react, or say something but not both.\n",
      "\n",
      "### Input:\n",
      "Name: Tommie (age: 25)\n",
      "Innate traits: anxious, likes design\n",
      "Tommie is an older man who has fond memories of his childhood pet, Bruno. He is a homebody and enjoys the comforts of home. Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "It is April 23, 2023, 10:13 PM.\n",
      "Tommie's status: looking for a job\n",
      "Summary of relevant context from Tommie's memory:\n",
      "Tommie is a person.\n",
      "```\n",
      "\n",
      "### Output:\n",
      "```\n",
      "What is the relationship between Tommie and Tommie?\n",
      "```\n",
      "Most recent observations: Tommie remembers his dog, Bruno, from when he was a kid; Tommie feels tired from driving so far; Tommie sees the new home; The new neighbors have a cat; The road is noisy at night; Tommie is hungry; Tommie tries to get some rest.; Tommie observed Beau says What do you like to do? and said \"what to say\".; Tommie observed Tommie wakes up to the sound of a noisy construction site outside his window. and reacted by Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.; Tommie observed Tommie gets out of bed and heads to the kitchen to make himself some coffee. and reacted by Tommie wakes up to the sound of a noisy construction site outside his window.; Tommie observed Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some. and reacted by ```; Tommie observed Tommie finally finds the filters and makes himself a cup of coffee. and reacted by ```; Tommie observed The coffee tastes bitter, and Tommie regrets not buying a better brand. and reacted by ```; Tommie observed Tommie checks his email and sees that he has no job offers yet. and reacted by ```\n",
      "Observation: Tommie spends some time updating his resume and cover letter.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie remembers brushing his teeth\n",
      "\n",
      "### Response:\n",
      "Rating: 1\n",
      "\n",
      "### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie observed Tommie spends some time updating his resume and cover letter. and reacted by ```\n",
      "\n",
      "### Response:\n",
      "Rating:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     6.45 ms /    18 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time = 12624.51 ms /   603 tokens (   20.94 ms per token)\n",
      "llama_print_timings:        eval time =  7416.92 ms /    17 runs   (  436.29 ms per run)\n",
      "llama_print_timings:       total time = 20049.20 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "0.015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     1.71 ms /     5 runs   (    0.34 ms per run)\n",
      "llama_print_timings: prompt eval time =  4035.45 ms /   233 tokens (   17.32 ms per token)\n",
      "llama_print_timings:        eval time =  1581.30 ms /     4 runs   (  395.32 ms per run)\n",
      "llama_print_timings:       total time =  5618.84 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2052.57 ms /    25 tokens (   82.10 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2052.82 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tommie spends some time updating his resume and cover letter. ```\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog is chasing a ball.\n",
      "\n",
      "### Response:\n",
      "The dog\n",
      "\n",
      "### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "Tommie heads out to explore the city and look for job openings.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What is The dog doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog jumps into the air and catches the ball.\n",
      "\n",
      "### Response:\n",
      "Catching a ball, possibly playing fetch\n",
      "\n",
      "### Instruction:\n",
      "What is Tommie doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "Tommie heads out to explore the city and look for job openings.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     3.56 ms /    10 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time =  2732.04 ms /    90 tokens (   30.36 ms per token)\n",
      "llama_print_timings:        eval time =  3409.85 ms /     9 runs   (  378.87 ms per run)\n",
      "llama_print_timings:       total time =  6146.09 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     4.36 ms /    12 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time =  2673.46 ms /   101 tokens (   26.47 ms per token)\n",
      "llama_print_timings:        eval time =  4185.23 ms /    11 runs   (  380.48 ms per run)\n",
      "llama_print_timings:       total time =  6863.86 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2369.50 ms /    30 tokens (   78.98 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2369.84 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =   783.63 ms /     9 tokens (   87.07 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =   783.92 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Given the provided information, what is the relationship between Tommie and Tommie? Be succinct.\n",
      "\n",
      "### Input:\n",
      "Context from memory:\n",
      "- April 23, 2023, 09:51 PM: Tommie tries to get some rest.\n",
      "- April 23, 2023, 09:50 PM: Tommie remembers his dog, Bruno, from when he was a kid\n",
      "- April 23, 2023, 10:01 PM: Tommie observed Tommie wakes up to the sound of a noisy construction site outside his window. and reacted by Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "- April 23, 2023, 09:51 PM: Tommie is hungry\n",
      "- April 23, 2023, 09:51 PM: The road is noisy at night\n",
      "- April 23, 2023, 09:51 PM: The new neighbors have a cat\n",
      "- April 23, 2023, 10:03 PM: Tommie observed Tommie gets out of bed and heads to the kitchen to make himself some coffee. and reacted by Tommie wakes up to the sound of a noisy construction site outside his window.\n",
      "- April 23, 2023, 10:13 PM: Tommie observed Tommie spends some time updating his resume and cover letter. and reacted by ```\n",
      "- April 23, 2023, 09:50 PM: Tommie feels tired from driving so far\n",
      "- April 23, 2023, 10:06 PM: Tommie observed Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some. and reacted by ```\n",
      "- April 23, 2023, 10:12 PM: Tommie observed Tommie checks his email and sees that he has no job offers yet. and reacted by ```\n",
      "- April 23, 2023, 10:09 PM: Tommie observed The coffee tastes bitter, and Tommie regrets not buying a better brand. and reacted by ```\n",
      "- April 23, 2023, 10:08 PM: Tommie observed Tommie finally finds the filters and makes himself a cup of coffee. and reacted by ```\n",
      "- April 23, 2023, 09:53 PM: Tommie observed Beau says What do you like to do? and said \"what to say\".\n",
      "- April 23, 2023, 09:50 PM: Tommie sees the new home\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =    69.84 ms /   191 runs   (    0.37 ms per run)\n",
      "llama_print_timings: prompt eval time = 13494.66 ms /   680 tokens (   19.85 ms per token)\n",
      "llama_print_timings:        eval time = 85861.71 ms /   190 runs   (  451.90 ms per run)\n",
      "llama_print_timings:       total time = 99448.11 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Should Tommie react to the observation, and if so, what would be an appropriate reaction? Respond in one line. If the action is to engage in dialogue, write:\n",
      "SAY: \"what to say\"\n",
      "otherwise, write:\n",
      "REACT: Tommie's reaction (if anything).\n",
      "Either do nothing, react, or say something but not both.\n",
      "\n",
      "### Input:\n",
      "Name: Tommie (age: 25)\n",
      "Innate traits: anxious, likes design\n",
      "Tommie is an older man who has fond memories of his childhood pet, Bruno. He is a homebody and enjoys the comforts of home. Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "It is April 23, 2023, 10:15 PM.\n",
      "Tommie's status: looking for a job\n",
      "Summary of relevant context from Tommie's memory:\n",
      "Tommie is a person. Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep. The new neighbors have a cat. Tommie gets out of bed and heads to the kitchen to make himself some coffee. Tommie wakes up to the sound of a noisy construction site outside his window. Tommie spends some time updating his resume and cover letter. Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some. Tommie checks his email and sees that he has no job offers yet. The coffee tastes bitter, and Tommie regrets not buying a better brand. Tommie finally finds the filters and makes himself a cup of coffee. Beau says What do you like to do? and said \"what to say\".\n",
      "```\n",
      "Most recent observations: Tommie remembers his dog, Bruno, from when he was a kid; Tommie feels tired from driving so far; Tommie sees the new home; The new neighbors have a cat; The road is noisy at night; Tommie is hungry; Tommie tries to get some rest.; Tommie observed Beau says What do you like to do? and said \"what to say\".; Tommie observed Tommie wakes up to the sound of a noisy construction site outside his window. and reacted by Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.; Tommie observed Tommie gets out of bed and heads to the kitchen to make himself some coffee. and reacted by Tommie wakes up to the sound of a noisy construction site outside his window.; Tommie observed Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some. and reacted by ```; Tommie observed Tommie finally finds the filters and makes himself a cup of coffee. and reacted by ```; Tommie observed The coffee tastes bitter, and Tommie regrets not buying a better brand. and reacted by ```; Tommie observed Tommie checks his email and sees that he has no job offers yet. and reacted by ```; Tommie observed Tommie spends some time updating his resume and cover letter. and reacted by ```\n",
      "Observation: Tommie heads out to explore the city and look for job openings.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie remembers brushing his teeth\n",
      "\n",
      "### Response:\n",
      "Rating: 1\n",
      "\n",
      "### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie observed Tommie heads out to explore the city and look for job openings. and reacted by ```\n",
      "\n",
      "### Response:\n",
      "Rating:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =    18.23 ms /    51 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time = 15481.13 ms /   786 tokens (   19.70 ms per token)\n",
      "llama_print_timings:        eval time = 22617.67 ms /    50 runs   (  452.35 ms per run)\n",
      "llama_print_timings:       total time = 38120.87 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "0.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     1.79 ms /     5 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time =  4053.19 ms /   234 tokens (   17.32 ms per token)\n",
      "llama_print_timings:        eval time =  1568.13 ms /     4 runs   (  392.03 ms per run)\n",
      "llama_print_timings:       total time =  5623.48 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tommie heads out to explore the city and look for job openings. ```\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog is chasing a ball.\n",
      "\n",
      "### Response:\n",
      "The dog\n",
      "\n",
      "### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "Tommie sees a sign for a job fair and decides to attend.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2097.91 ms /    26 tokens (   80.69 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2098.18 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What is The dog doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog jumps into the air and catches the ball.\n",
      "\n",
      "### Response:\n",
      "Catching a ball, possibly playing fetch\n",
      "\n",
      "### Instruction:\n",
      "What is Tommie doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "Tommie sees a sign for a job fair and decides to attend.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     3.63 ms /    10 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time =  2748.14 ms /    90 tokens (   30.53 ms per token)\n",
      "llama_print_timings:        eval time =  3421.31 ms /     9 runs   (  380.15 ms per run)\n",
      "llama_print_timings:       total time =  6173.76 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     4.47 ms /    12 runs   (    0.37 ms per run)\n",
      "llama_print_timings: prompt eval time =  2681.79 ms /   101 tokens (   26.55 ms per token)\n",
      "llama_print_timings:        eval time =  4178.29 ms /    11 runs   (  379.84 ms per run)\n",
      "llama_print_timings:       total time =  6865.35 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2450.95 ms /    30 tokens (   81.70 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2451.28 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =   852.75 ms /    10 tokens (   85.28 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =   852.91 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Given the provided information, what is the relationship between Tommie and Tommie? Be succinct.\n",
      "\n",
      "### Input:\n",
      "Context from memory:\n",
      "- April 23, 2023, 09:51 PM: Tommie tries to get some rest.\n",
      "- April 23, 2023, 09:50 PM: Tommie remembers his dog, Bruno, from when he was a kid\n",
      "- April 23, 2023, 10:01 PM: Tommie observed Tommie wakes up to the sound of a noisy construction site outside his window. and reacted by Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "- April 23, 2023, 09:51 PM: Tommie is hungry\n",
      "- April 23, 2023, 09:51 PM: The road is noisy at night\n",
      "- April 23, 2023, 09:51 PM: The new neighbors have a cat\n",
      "- April 23, 2023, 10:03 PM: Tommie observed Tommie gets out of bed and heads to the kitchen to make himself some coffee. and reacted by Tommie wakes up to the sound of a noisy construction site outside his window.\n",
      "- April 23, 2023, 10:13 PM: Tommie observed Tommie spends some time updating his resume and cover letter. and reacted by ```\n",
      "- April 23, 2023, 09:50 PM: Tommie feels tired from driving so far\n",
      "- April 23, 2023, 10:16 PM: Tommie observed Tommie heads out to explore the city and look for job openings. and reacted by ```\n",
      "- April 23, 2023, 10:06 PM: Tommie observed Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some. and reacted by ```\n",
      "- April 23, 2023, 10:12 PM: Tommie observed Tommie checks his email and sees that he has no job offers yet. and reacted by ```\n",
      "- April 23, 2023, 10:09 PM: Tommie observed The coffee tastes bitter, and Tommie regrets not buying a better brand. and reacted by ```\n",
      "- April 23, 2023, 10:08 PM: Tommie observed Tommie finally finds the filters and makes himself a cup of coffee. and reacted by ```\n",
      "- April 23, 2023, 09:53 PM: Tommie observed Beau says What do you like to do? and said \"what to say\".\n",
      "- April 23, 2023, 09:50 PM: Tommie sees the new home\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =    12.22 ms /    34 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time = 14348.80 ms /   726 tokens (   19.76 ms per token)\n",
      "llama_print_timings:        eval time = 14810.86 ms /    33 runs   (  448.81 ms per run)\n",
      "llama_print_timings:       total time = 29174.48 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Should Tommie react to the observation, and if so, what would be an appropriate reaction? Respond in one line. If the action is to engage in dialogue, write:\n",
      "SAY: \"what to say\"\n",
      "otherwise, write:\n",
      "REACT: Tommie's reaction (if anything).\n",
      "Either do nothing, react, or say something but not both.\n",
      "\n",
      "### Input:\n",
      "Name: Tommie (age: 25)\n",
      "Innate traits: anxious, likes design\n",
      "Tommie is an older man who has fond memories of his childhood pet, Bruno. He is a homebody and enjoys the comforts of home. Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "It is April 23, 2023, 10:17 PM.\n",
      "Tommie's status: looking for a job\n",
      "Summary of relevant context from Tommie's memory:\n",
      "Tommie is a person.\n",
      "```\n",
      "\n",
      "### Output:\n",
      "The relationship between Tommie and Tommie is that they are both people.\n",
      "Most recent observations: Tommie remembers his dog, Bruno, from when he was a kid; Tommie feels tired from driving so far; Tommie sees the new home; The new neighbors have a cat; The road is noisy at night; Tommie is hungry; Tommie tries to get some rest.; Tommie observed Beau says What do you like to do? and said \"what to say\".; Tommie observed Tommie wakes up to the sound of a noisy construction site outside his window. and reacted by Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.; Tommie observed Tommie gets out of bed and heads to the kitchen to make himself some coffee. and reacted by Tommie wakes up to the sound of a noisy construction site outside his window.; Tommie observed Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some. and reacted by ```; Tommie observed Tommie finally finds the filters and makes himself a cup of coffee. and reacted by ```; Tommie observed The coffee tastes bitter, and Tommie regrets not buying a better brand. and reacted by ```; Tommie observed Tommie checks his email and sees that he has no job offers yet. and reacted by ```; Tommie observed Tommie spends some time updating his resume and cover letter. and reacted by ```; Tommie observed Tommie heads out to explore the city and look for job openings. and reacted by ```\n",
      "Observation: Tommie sees a sign for a job fair and decides to attend.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     6.74 ms /    18 runs   (    0.37 ms per run)\n",
      "llama_print_timings: prompt eval time = 13437.17 ms /   655 tokens (   20.51 ms per token)\n",
      "llama_print_timings:        eval time =  7489.16 ms /    17 runs   (  440.54 ms per run)\n",
      "llama_print_timings:       total time = 20934.46 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie remembers brushing his teeth\n",
      "\n",
      "### Response:\n",
      "Rating: 1\n",
      "\n",
      "### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie observed Tommie sees a sign for a job fair and decides to attend. and reacted by ```\n",
      "\n",
      "### Response:\n",
      "Rating:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     1.84 ms /     5 runs   (    0.37 ms per run)\n",
      "llama_print_timings: prompt eval time =  4031.15 ms /   234 tokens (   17.23 ms per token)\n",
      "llama_print_timings:        eval time =  1576.05 ms /     4 runs   (  394.01 ms per run)\n",
      "llama_print_timings:       total time =  5609.40 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "0.015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2080.33 ms /    26 tokens (   80.01 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2080.64 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tommie sees a sign for a job fair and decides to attend. ```\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog is chasing a ball.\n",
      "\n",
      "### Response:\n",
      "The dog\n",
      "\n",
      "### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The line to get in is long, and Tommie has to wait for an hour.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What is The dog doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog jumps into the air and catches the ball.\n",
      "\n",
      "### Response:\n",
      "Catching a ball, possibly playing fetch\n",
      "\n",
      "### Instruction:\n",
      "What is Tommie doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The line to get in is long, and Tommie has to wait for an hour.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     3.62 ms /    10 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time =  2803.07 ms /    93 tokens (   30.14 ms per token)\n",
      "llama_print_timings:        eval time =  3398.89 ms /     9 runs   (  377.65 ms per run)\n",
      "llama_print_timings:       total time =  6206.27 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     3.88 ms /    11 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time =  2711.34 ms /   104 tokens (   26.07 ms per token)\n",
      "llama_print_timings:        eval time =  3778.12 ms /    10 runs   (  377.81 ms per run)\n",
      "llama_print_timings:       total time =  6494.05 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2370.56 ms /    30 tokens (   79.02 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2370.91 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =   759.27 ms /     9 tokens (   84.36 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =   759.42 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Given the provided information, what is the relationship between Tommie and Tommie? Be succinct.\n",
      "\n",
      "### Input:\n",
      "Context from memory:\n",
      "- April 23, 2023, 09:51 PM: Tommie tries to get some rest.\n",
      "- April 23, 2023, 09:50 PM: Tommie remembers his dog, Bruno, from when he was a kid\n",
      "- April 23, 2023, 10:01 PM: Tommie observed Tommie wakes up to the sound of a noisy construction site outside his window. and reacted by Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "- April 23, 2023, 09:51 PM: Tommie is hungry\n",
      "- April 23, 2023, 09:51 PM: The road is noisy at night\n",
      "- April 23, 2023, 09:51 PM: The new neighbors have a cat\n",
      "- April 23, 2023, 10:03 PM: Tommie observed Tommie gets out of bed and heads to the kitchen to make himself some coffee. and reacted by Tommie wakes up to the sound of a noisy construction site outside his window.\n",
      "- April 23, 2023, 10:13 PM: Tommie observed Tommie spends some time updating his resume and cover letter. and reacted by ```\n",
      "- April 23, 2023, 09:50 PM: Tommie feels tired from driving so far\n",
      "- April 23, 2023, 10:16 PM: Tommie observed Tommie heads out to explore the city and look for job openings. and reacted by ```\n",
      "- April 23, 2023, 10:06 PM: Tommie observed Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some. and reacted by ```\n",
      "- April 23, 2023, 10:12 PM: Tommie observed Tommie checks his email and sees that he has no job offers yet. and reacted by ```\n",
      "- April 23, 2023, 10:09 PM: Tommie observed The coffee tastes bitter, and Tommie regrets not buying a better brand. and reacted by ```\n",
      "- April 23, 2023, 10:17 PM: Tommie observed Tommie sees a sign for a job fair and decides to attend. and reacted by ```\n",
      "- April 23, 2023, 10:08 PM: Tommie observed Tommie finally finds the filters and makes himself a cup of coffee. and reacted by ```\n",
      "- April 23, 2023, 09:50 PM: Tommie sees the new home\n",
      "- April 23, 2023, 09:53 PM: Tommie observed Beau says What do you like to do? and said \"what to say\".\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =    92.42 ms /   256 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time = 15007.60 ms /   772 tokens (   19.44 ms per token)\n",
      "llama_print_timings:        eval time = 116959.06 ms /   255 runs   (  458.66 ms per run)\n",
      "llama_print_timings:       total time = 132088.54 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Should Tommie react to the observation, and if so, what would be an appropriate reaction? Respond in one line. If the action is to engage in dialogue, write:\n",
      "SAY: \"what to say\"\n",
      "otherwise, write:\n",
      "REACT: Tommie's reaction (if anything).\n",
      "Either do nothing, react, or say something but not both.\n",
      "\n",
      "### Input:\n",
      "Name: Tommie (age: 25)\n",
      "Innate traits: anxious, likes design\n",
      "Tommie is an older man who has fond memories of his childhood pet, Bruno. He is a homebody and enjoys the comforts of home. Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "It is April 23, 2023, 10:20 PM.\n",
      "Tommie's status: looking for a job\n",
      "Summary of relevant context from Tommie's memory:\n",
      "Tommie is a person.\n",
      "```\n",
      "\n",
      "## Instruction:\n",
      "Given the provided information, what is the relationship between Tommie and Tommie? Be succinct.\n",
      "\n",
      "### Input:\n",
      "Context from memory:\n",
      "- April 23, 2023, 09:51 PM: Tommie tries to get some rest.\n",
      "- April 23, 2023, 09:50 PM: Tommie remembers his dog, Bruno, from when he was a kid\n",
      "- April 23, 2023, 10:01 PM: Tommie observed Tommie wakes up to the sound of a noisy construction site outside his window. and reacted by Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "- April 23, 2023, 09:51 PM: Tommie is hungry\n",
      "- April 23, 2023, 09:51 PM: The road is noisy at night\n",
      "- April 23\n",
      "Most recent observations: Tommie remembers his dog, Bruno, from when he was a kid; Tommie feels tired from driving so far; Tommie sees the new home; The new neighbors have a cat; The road is noisy at night; Tommie is hungry; Tommie tries to get some rest.; Tommie observed Beau says What do you like to do? and said \"what to say\".; Tommie observed Tommie wakes up to the sound of a noisy construction site outside his window. and reacted by Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.; Tommie observed Tommie gets out of bed and heads to the kitchen to make himself some coffee. and reacted by Tommie wakes up to the sound of a noisy construction site outside his window.; Tommie observed Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some. and reacted by ```; Tommie observed Tommie finally finds the filters and makes himself a cup of coffee. and reacted by ```; Tommie observed The coffee tastes bitter, and Tommie regrets not buying a better brand. and reacted by ```; Tommie observed Tommie checks his email and sees that he has no job offers yet. and reacted by ```; Tommie observed Tommie spends some time updating his resume and cover letter. and reacted by ```; Tommie observed Tommie heads out to explore the city and look for job openings. and reacted by ```; Tommie observed Tommie sees a sign for a job fair and decides to attend. and reacted by ```\n",
      "Observation: The line to get in is long, and Tommie has to wait for an hour.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie remembers brushing his teeth\n",
      "\n",
      "### Response:\n",
      "Rating: 1\n",
      "\n",
      "### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie observed The line to get in is long, and Tommie has to wait for an hour. and reacted by ```\n",
      "\n",
      "### Response:\n",
      "Rating:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     6.41 ms /    18 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time = 18248.24 ms /   907 tokens (   20.12 ms per token)\n",
      "llama_print_timings:        eval time =  7755.13 ms /    17 runs   (  456.18 ms per run)\n",
      "llama_print_timings:       total time = 26011.09 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "0.015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     1.74 ms /     5 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time =  3982.74 ms /   237 tokens (   16.80 ms per token)\n",
      "llama_print_timings:        eval time =  1555.05 ms /     4 runs   (  388.76 ms per run)\n",
      "llama_print_timings:       total time =  5539.92 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2274.07 ms /    29 tokens (   78.42 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2274.39 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line to get in is long, and Tommie has to wait for an hour. ```\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog is chasing a ball.\n",
      "\n",
      "### Response:\n",
      "The dog\n",
      "\n",
      "### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "Tommie meets several potential employers at the job fair but doesn't receive any offers.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What is The dog doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog jumps into the air and catches the ball.\n",
      "\n",
      "### Response:\n",
      "Catching a ball, possibly playing fetch\n",
      "\n",
      "### Instruction:\n",
      "What is Tommie doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "Tommie meets several potential employers at the job fair but doesn't receive any offers.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     3.49 ms /    10 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time =  2713.37 ms /    94 tokens (   28.87 ms per token)\n",
      "llama_print_timings:        eval time =  3372.51 ms /     9 runs   (  374.72 ms per run)\n",
      "llama_print_timings:       total time =  6090.02 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     5.76 ms /    16 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time =  2646.80 ms /   105 tokens (   25.21 ms per token)\n",
      "llama_print_timings:        eval time =  5636.53 ms /    15 runs   (  375.77 ms per run)\n",
      "llama_print_timings:       total time =  8290.10 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2356.98 ms /    30 tokens (   78.57 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2357.31 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  1156.43 ms /    14 tokens (   82.60 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  1156.63 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Given the provided information, what is the relationship between Tommie and Tommie? Be succinct.\n",
      "\n",
      "### Input:\n",
      "Context from memory:\n",
      "- April 23, 2023, 09:51 PM: Tommie tries to get some rest.\n",
      "- April 23, 2023, 09:50 PM: Tommie remembers his dog, Bruno, from when he was a kid\n",
      "- April 23, 2023, 10:01 PM: Tommie observed Tommie wakes up to the sound of a noisy construction site outside his window. and reacted by Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "- April 23, 2023, 09:51 PM: Tommie is hungry\n",
      "- April 23, 2023, 09:51 PM: The road is noisy at night\n",
      "- April 23, 2023, 09:51 PM: The new neighbors have a cat\n",
      "- April 23, 2023, 10:03 PM: Tommie observed Tommie gets out of bed and heads to the kitchen to make himself some coffee. and reacted by Tommie wakes up to the sound of a noisy construction site outside his window.\n",
      "- April 23, 2023, 10:13 PM: Tommie observed Tommie spends some time updating his resume and cover letter. and reacted by ```\n",
      "- April 23, 2023, 09:50 PM: Tommie feels tired from driving so far\n",
      "- April 23, 2023, 10:16 PM: Tommie observed Tommie heads out to explore the city and look for job openings. and reacted by ```\n",
      "- April 23, 2023, 10:06 PM: Tommie observed Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some. and reacted by ```\n",
      "- April 23, 2023, 10:12 PM: Tommie observed Tommie checks his email and sees that he has no job offers yet. and reacted by ```\n",
      "- April 23, 2023, 10:09 PM: Tommie observed The coffee tastes bitter, and Tommie regrets not buying a better brand. and reacted by ```\n",
      "- April 23, 2023, 10:17 PM: Tommie observed Tommie sees a sign for a job fair and decides to attend. and reacted by ```\n",
      "- April 23, 2023, 10:08 PM: Tommie observed Tommie finally finds the filters and makes himself a cup of coffee. and reacted by ```\n",
      "- April 23, 2023, 09:50 PM: Tommie sees the new home\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =    23.84 ms /    67 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time = 14178.58 ms /   731 tokens (   19.40 ms per token)\n",
      "llama_print_timings:        eval time = 29540.17 ms /    66 runs   (  447.58 ms per run)\n",
      "llama_print_timings:       total time = 43747.79 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Should Tommie react to the observation, and if so, what would be an appropriate reaction? Respond in one line. If the action is to engage in dialogue, write:\n",
      "SAY: \"what to say\"\n",
      "otherwise, write:\n",
      "REACT: Tommie's reaction (if anything).\n",
      "Either do nothing, react, or say something but not both.\n",
      "\n",
      "### Input:\n",
      "Name: Tommie (age: 25)\n",
      "Innate traits: anxious, likes design\n",
      "Tommie is an older man who has fond memories of his childhood pet, Bruno. He is a homebody and enjoys the comforts of home. Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "It is April 23, 2023, 10:21 PM.\n",
      "Tommie's status: looking for a job\n",
      "Summary of relevant context from Tommie's memory:\n",
      "Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "```\n",
      "\n",
      "### Output:\n",
      "The relationship between Tommie and Tommie is that they are both the same person, but with different contexts.\n",
      "```\n",
      "Most recent observations: Tommie remembers his dog, Bruno, from when he was a kid; Tommie feels tired from driving so far; Tommie sees the new home; The new neighbors have a cat; The road is noisy at night; Tommie is hungry; Tommie tries to get some rest.; Tommie observed Beau says What do you like to do? and said \"what to say\".; Tommie observed Tommie wakes up to the sound of a noisy construction site outside his window. and reacted by Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.; Tommie observed Tommie gets out of bed and heads to the kitchen to make himself some coffee. and reacted by Tommie wakes up to the sound of a noisy construction site outside his window.; Tommie observed Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some. and reacted by ```; Tommie observed Tommie finally finds the filters and makes himself a cup of coffee. and reacted by ```; Tommie observed The coffee tastes bitter, and Tommie regrets not buying a better brand. and reacted by ```; Tommie observed Tommie checks his email and sees that he has no job offers yet. and reacted by ```; Tommie observed Tommie spends some time updating his resume and cover letter. and reacted by ```; Tommie observed Tommie heads out to explore the city and look for job openings. and reacted by ```; Tommie observed Tommie sees a sign for a job fair and decides to attend. and reacted by ```; Tommie observed The line to get in is long, and Tommie has to wait for an hour. and reacted by ```\n",
      "Observation: Tommie meets several potential employers at the job fair but doesn't receive any offers.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie remembers brushing his teeth\n",
      "\n",
      "### Response:\n",
      "Rating: 1\n",
      "\n",
      "### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie observed Tommie meets several potential employers at the job fair but doesn't receive any offers. and reacted by ```\n",
      "\n",
      "### Response:\n",
      "Rating:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =    11.25 ms /    31 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time = 14752.18 ms /   747 tokens (   19.75 ms per token)\n",
      "llama_print_timings:        eval time = 13544.50 ms /    30 runs   (  451.48 ms per run)\n",
      "llama_print_timings:       total time = 28310.21 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "0.015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     2.14 ms /     6 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time =  4050.30 ms /   238 tokens (   17.02 ms per token)\n",
      "llama_print_timings:        eval time =  2008.99 ms /     5 runs   (  401.80 ms per run)\n",
      "llama_print_timings:       total time =  6061.92 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tommie meets several potential employers at the job fair but doesn't receive any offers. ```\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog is chasing a ball.\n",
      "\n",
      "### Response:\n",
      "The dog\n",
      "\n",
      "### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "Tommie leaves the job fair feeling disappointed.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2418.19 ms /    30 tokens (   80.61 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2418.51 ms\n",
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     3.68 ms /    10 runs   (    0.37 ms per run)\n",
      "llama_print_timings: prompt eval time =  2757.29 ms /    85 tokens (   32.44 ms per token)\n",
      "llama_print_timings:        eval time =  3602.35 ms /     9 runs   (  400.26 ms per run)\n",
      "llama_print_timings:       total time =  6364.12 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What is The dog doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog jumps into the air and catches the ball.\n",
      "\n",
      "### Response:\n",
      "Catching a ball, possibly playing fetch\n",
      "\n",
      "### Instruction:\n",
      "What is Tommie doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "Tommie leaves the job fair feeling disappointed.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     5.78 ms /    16 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time =  2736.93 ms /    96 tokens (   28.51 ms per token)\n",
      "llama_print_timings:        eval time =  5769.95 ms /    15 runs   (  384.66 ms per run)\n",
      "llama_print_timings:       total time =  8513.82 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2384.24 ms /    30 tokens (   79.47 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2384.53 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  1132.67 ms /    14 tokens (   80.91 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  1132.86 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Given the provided information, what is the relationship between Tommie and Tommie? Be succinct.\n",
      "\n",
      "### Input:\n",
      "Context from memory:\n",
      "- April 23, 2023, 09:51 PM: Tommie tries to get some rest.\n",
      "- April 23, 2023, 09:50 PM: Tommie remembers his dog, Bruno, from when he was a kid\n",
      "- April 23, 2023, 10:01 PM: Tommie observed Tommie wakes up to the sound of a noisy construction site outside his window. and reacted by Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "- April 23, 2023, 09:51 PM: Tommie is hungry\n",
      "- April 23, 2023, 09:51 PM: The road is noisy at night\n",
      "- April 23, 2023, 09:51 PM: The new neighbors have a cat\n",
      "- April 23, 2023, 10:03 PM: Tommie observed Tommie gets out of bed and heads to the kitchen to make himself some coffee. and reacted by Tommie wakes up to the sound of a noisy construction site outside his window.\n",
      "- April 23, 2023, 10:13 PM: Tommie observed Tommie spends some time updating his resume and cover letter. and reacted by ```\n",
      "- April 23, 2023, 09:50 PM: Tommie feels tired from driving so far\n",
      "- April 23, 2023, 10:16 PM: Tommie observed Tommie heads out to explore the city and look for job openings. and reacted by ```\n",
      "- April 23, 2023, 10:06 PM: Tommie observed Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some. and reacted by ```\n",
      "- April 23, 2023, 10:12 PM: Tommie observed Tommie checks his email and sees that he has no job offers yet. and reacted by ```\n",
      "- April 23, 2023, 10:09 PM: Tommie observed The coffee tastes bitter, and Tommie regrets not buying a better brand. and reacted by ```\n",
      "- April 23, 2023, 10:17 PM: Tommie observed Tommie sees a sign for a job fair and decides to attend. and reacted by ```\n",
      "- April 23, 2023, 10:08 PM: Tommie observed Tommie finally finds the filters and makes himself a cup of coffee. and reacted by ```\n",
      "- April 23, 2023, 09:50 PM: Tommie sees the new home\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =    24.25 ms /    67 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time = 14347.82 ms /   731 tokens (   19.63 ms per token)\n",
      "llama_print_timings:        eval time = 30670.07 ms /    66 runs   (  464.70 ms per run)\n",
      "llama_print_timings:       total time = 45048.06 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Should Tommie react to the observation, and if so, what would be an appropriate reaction? Respond in one line. If the action is to engage in dialogue, write:\n",
      "SAY: \"what to say\"\n",
      "otherwise, write:\n",
      "REACT: Tommie's reaction (if anything).\n",
      "Either do nothing, react, or say something but not both.\n",
      "\n",
      "### Input:\n",
      "Name: Tommie (age: 25)\n",
      "Innate traits: anxious, likes design\n",
      "Tommie is an older man who has fond memories of his childhood pet, Bruno. He is a homebody and enjoys the comforts of home. Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "It is April 23, 2023, 10:23 PM.\n",
      "Tommie's status: looking for a job\n",
      "Summary of relevant context from Tommie's memory:\n",
      "Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "```\n",
      "\n",
      "### Output:\n",
      "The relationship between Tommie and Tommie is that they are both the same person, but with different contexts.\n",
      "```\n",
      "Most recent observations: Tommie remembers his dog, Bruno, from when he was a kid; Tommie feels tired from driving so far; Tommie sees the new home; The new neighbors have a cat; The road is noisy at night; Tommie is hungry; Tommie tries to get some rest.; Tommie observed Beau says What do you like to do? and said \"what to say\".; Tommie observed Tommie wakes up to the sound of a noisy construction site outside his window. and reacted by Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.; Tommie observed Tommie gets out of bed and heads to the kitchen to make himself some coffee. and reacted by Tommie wakes up to the sound of a noisy construction site outside his window.; Tommie observed Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some. and reacted by ```; Tommie observed Tommie finally finds the filters and makes himself a cup of coffee. and reacted by ```; Tommie observed The coffee tastes bitter, and Tommie regrets not buying a better brand. and reacted by ```; Tommie observed Tommie checks his email and sees that he has no job offers yet. and reacted by ```; Tommie observed Tommie spends some time updating his resume and cover letter. and reacted by ```; Tommie observed Tommie heads out to explore the city and look for job openings. and reacted by ```; Tommie observed Tommie sees a sign for a job fair and decides to attend. and reacted by ```; Tommie observed The line to get in is long, and Tommie has to wait for an hour. and reacted by ```; Tommie observed Tommie meets several potential employers at the job fair but doesn't receive any offers. and reacted by ```\n",
      "Observation: Tommie leaves the job fair feeling disappointed.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie remembers brushing his teeth\n",
      "\n",
      "### Response:\n",
      "Rating: 1\n",
      "\n",
      "### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie observed Tommie leaves the job fair feeling disappointed. and reacted by ```\n",
      "\n",
      "### Response:\n",
      "Rating:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =    86.64 ms /   236 runs   (    0.37 ms per run)\n",
      "llama_print_timings: prompt eval time = 14823.22 ms /   768 tokens (   19.30 ms per token)\n",
      "llama_print_timings:        eval time = 111368.22 ms /   235 runs   (  473.91 ms per run)\n",
      "llama_print_timings:       total time = 126308.49 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "0.015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     2.25 ms /     6 runs   (    0.38 ms per run)\n",
      "llama_print_timings: prompt eval time =  4042.74 ms /   229 tokens (   17.65 ms per token)\n",
      "llama_print_timings:        eval time =  2030.07 ms /     5 runs   (  406.01 ms per run)\n",
      "llama_print_timings:       total time =  6075.57 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  1817.41 ms /    21 tokens (   86.54 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  1817.75 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tommie leaves the job fair feeling disappointed. ```\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog is chasing a ball.\n",
      "\n",
      "### Response:\n",
      "The dog\n",
      "\n",
      "### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "Tommie stops by a local diner to grab some lunch.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What is The dog doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog jumps into the air and catches the ball.\n",
      "\n",
      "### Response:\n",
      "Catching a ball, possibly playing fetch\n",
      "\n",
      "### Instruction:\n",
      "What is Tommie doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "Tommie stops by a local diner to grab some lunch.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     3.74 ms /    10 runs   (    0.37 ms per run)\n",
      "llama_print_timings: prompt eval time =  2946.05 ms /    89 tokens (   33.10 ms per token)\n",
      "llama_print_timings:        eval time =  3620.84 ms /     9 runs   (  402.31 ms per run)\n",
      "llama_print_timings:       total time =  6571.54 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     4.23 ms /    12 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time =  2677.93 ms /   100 tokens (   26.78 ms per token)\n",
      "llama_print_timings:        eval time =  4146.89 ms /    11 runs   (  376.99 ms per run)\n",
      "llama_print_timings:       total time =  6829.93 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2364.50 ms /    30 tokens (   78.82 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2364.83 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =   843.69 ms /    10 tokens (   84.37 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =   843.85 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Given the provided information, what is the relationship between Tommie and Tommie? Be succinct.\n",
      "\n",
      "### Input:\n",
      "Context from memory:\n",
      "- April 23, 2023, 09:51 PM: Tommie tries to get some rest.\n",
      "- April 23, 2023, 09:50 PM: Tommie remembers his dog, Bruno, from when he was a kid\n",
      "- April 23, 2023, 10:01 PM: Tommie observed Tommie wakes up to the sound of a noisy construction site outside his window. and reacted by Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "- April 23, 2023, 09:51 PM: Tommie is hungry\n",
      "- April 23, 2023, 09:51 PM: The road is noisy at night\n",
      "- April 23, 2023, 09:51 PM: The new neighbors have a cat\n",
      "- April 23, 2023, 10:03 PM: Tommie observed Tommie gets out of bed and heads to the kitchen to make himself some coffee. and reacted by Tommie wakes up to the sound of a noisy construction site outside his window.\n",
      "- April 23, 2023, 10:13 PM: Tommie observed Tommie spends some time updating his resume and cover letter. and reacted by ```\n",
      "- April 23, 2023, 09:50 PM: Tommie feels tired from driving so far\n",
      "- April 23, 2023, 10:16 PM: Tommie observed Tommie heads out to explore the city and look for job openings. and reacted by ```\n",
      "- April 23, 2023, 10:06 PM: Tommie observed Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some. and reacted by ```\n",
      "- April 23, 2023, 10:12 PM: Tommie observed Tommie checks his email and sees that he has no job offers yet. and reacted by ```\n",
      "- April 23, 2023, 10:25 PM: Tommie observed Tommie leaves the job fair feeling disappointed. and reacted by ```\n",
      "- April 23, 2023, 10:09 PM: Tommie observed The coffee tastes bitter, and Tommie regrets not buying a better brand. and reacted by ```\n",
      "- April 23, 2023, 10:17 PM: Tommie observed Tommie sees a sign for a job fair and decides to attend. and reacted by ```\n",
      "- April 23, 2023, 09:50 PM: Tommie sees the new home\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =    23.91 ms /    67 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time = 14220.34 ms /   727 tokens (   19.56 ms per token)\n",
      "llama_print_timings:        eval time = 29171.91 ms /    66 runs   (  442.00 ms per run)\n",
      "llama_print_timings:       total time = 43421.00 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Should Tommie react to the observation, and if so, what would be an appropriate reaction? Respond in one line. If the action is to engage in dialogue, write:\n",
      "SAY: \"what to say\"\n",
      "otherwise, write:\n",
      "REACT: Tommie's reaction (if anything).\n",
      "Either do nothing, react, or say something but not both.\n",
      "\n",
      "### Input:\n",
      "Name: Tommie (age: 25)\n",
      "Innate traits: anxious, likes design\n",
      "Tommie is an older man who has fond memories of his childhood pet, Bruno. He is a homebody and enjoys the comforts of home. Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "It is April 23, 2023, 10:26 PM.\n",
      "Tommie's status: looking for a job\n",
      "Summary of relevant context from Tommie's memory:\n",
      "Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "```\n",
      "\n",
      "### Output:\n",
      "The relationship between Tommie and Tommie is that they are both the same person, but in different contexts.\n",
      "```\n",
      "Most recent observations: Tommie remembers his dog, Bruno, from when he was a kid; Tommie feels tired from driving so far; Tommie sees the new home; The new neighbors have a cat; The road is noisy at night; Tommie is hungry; Tommie tries to get some rest.; Tommie observed Beau says What do you like to do? and said \"what to say\".; Tommie observed Tommie wakes up to the sound of a noisy construction site outside his window. and reacted by Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.; Tommie observed Tommie gets out of bed and heads to the kitchen to make himself some coffee. and reacted by Tommie wakes up to the sound of a noisy construction site outside his window.; Tommie observed Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some. and reacted by ```; Tommie observed Tommie finally finds the filters and makes himself a cup of coffee. and reacted by ```; Tommie observed The coffee tastes bitter, and Tommie regrets not buying a better brand. and reacted by ```; Tommie observed Tommie checks his email and sees that he has no job offers yet. and reacted by ```; Tommie observed Tommie spends some time updating his resume and cover letter. and reacted by ```; Tommie observed Tommie heads out to explore the city and look for job openings. and reacted by ```; Tommie observed Tommie sees a sign for a job fair and decides to attend. and reacted by ```; Tommie observed The line to get in is long, and Tommie has to wait for an hour. and reacted by ```; Tommie observed Tommie meets several potential employers at the job fair but doesn't receive any offers. and reacted by ```; Tommie observed Tommie leaves the job fair feeling disappointed. and reacted by ```\n",
      "Observation: Tommie stops by a local diner to grab some lunch.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =    15.12 ms /    18 runs   (    0.84 ms per run)\n",
      "llama_print_timings: prompt eval time = 21997.69 ms /   793 tokens (   27.74 ms per token)\n",
      "llama_print_timings:        eval time =  7593.10 ms /    17 runs   (  446.65 ms per run)\n",
      "llama_print_timings:       total time = 29612.97 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie remembers brushing his teeth\n",
      "\n",
      "### Response:\n",
      "Rating: 1\n",
      "\n",
      "### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie observed Tommie stops by a local diner to grab some lunch. and reacted by ```\n",
      "\n",
      "### Response:\n",
      "Rating:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "0.015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     1.83 ms /     5 runs   (    0.37 ms per run)\n",
      "llama_print_timings: prompt eval time =  3983.74 ms /   233 tokens (   17.10 ms per token)\n",
      "llama_print_timings:        eval time =  1587.44 ms /     4 runs   (  396.86 ms per run)\n",
      "llama_print_timings:       total time =  5573.62 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2789.23 ms /    25 tokens (  111.57 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2789.68 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tommie stops by a local diner to grab some lunch. ```\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog is chasing a ball.\n",
      "\n",
      "### Response:\n",
      "The dog\n",
      "\n",
      "### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The service is slow, and Tommie has to wait for 30 minutes to get his food.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What is The dog doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog jumps into the air and catches the ball.\n",
      "\n",
      "### Response:\n",
      "Catching a ball, possibly playing fetch\n",
      "\n",
      "### Instruction:\n",
      "What is Tommie doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The service is slow, and Tommie has to wait for 30 minutes to get his food.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     3.68 ms /    10 runs   (    0.37 ms per run)\n",
      "llama_print_timings: prompt eval time =  2842.54 ms /    96 tokens (   29.61 ms per token)\n",
      "llama_print_timings:        eval time =  3468.47 ms /     9 runs   (  385.39 ms per run)\n",
      "llama_print_timings:       total time =  6315.43 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     5.46 ms /    15 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time =  2736.93 ms /   107 tokens (   25.58 ms per token)\n",
      "llama_print_timings:        eval time =  5514.00 ms /    14 runs   (  393.86 ms per run)\n",
      "llama_print_timings:       total time =  8257.56 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2489.13 ms /    30 tokens (   82.97 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2489.58 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  1244.96 ms /    13 tokens (   95.77 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  1245.36 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Given the provided information, what is the relationship between Tommie and Tommie? Be succinct.\n",
      "\n",
      "### Input:\n",
      "Context from memory:\n",
      "- April 23, 2023, 09:51 PM: Tommie tries to get some rest.\n",
      "- April 23, 2023, 09:50 PM: Tommie remembers his dog, Bruno, from when he was a kid\n",
      "- April 23, 2023, 10:01 PM: Tommie observed Tommie wakes up to the sound of a noisy construction site outside his window. and reacted by Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "- April 23, 2023, 09:51 PM: Tommie is hungry\n",
      "- April 23, 2023, 09:51 PM: The road is noisy at night\n",
      "- April 23, 2023, 09:51 PM: The new neighbors have a cat\n",
      "- April 23, 2023, 10:03 PM: Tommie observed Tommie gets out of bed and heads to the kitchen to make himself some coffee. and reacted by Tommie wakes up to the sound of a noisy construction site outside his window.\n",
      "- April 23, 2023, 10:13 PM: Tommie observed Tommie spends some time updating his resume and cover letter. and reacted by ```\n",
      "- April 23, 2023, 09:50 PM: Tommie feels tired from driving so far\n",
      "- April 23, 2023, 10:16 PM: Tommie observed Tommie heads out to explore the city and look for job openings. and reacted by ```\n",
      "- April 23, 2023, 10:06 PM: Tommie observed Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some. and reacted by ```\n",
      "- April 23, 2023, 10:12 PM: Tommie observed Tommie checks his email and sees that he has no job offers yet. and reacted by ```\n",
      "- April 23, 2023, 10:09 PM: Tommie observed The coffee tastes bitter, and Tommie regrets not buying a better brand. and reacted by ```\n",
      "- April 23, 2023, 10:25 PM: Tommie observed Tommie leaves the job fair feeling disappointed. and reacted by ```\n",
      "- April 23, 2023, 10:17 PM: Tommie observed Tommie sees a sign for a job fair and decides to attend. and reacted by ```\n",
      "- April 23, 2023, 09:50 PM: Tommie sees the new home\n",
      "- April 23, 2023, 10:27 PM: Tommie observed Tommie stops by a local diner to grab some lunch. and reacted by ```\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =    24.43 ms /    67 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time = 15322.77 ms /   772 tokens (   19.85 ms per token)\n",
      "llama_print_timings:        eval time = 29945.88 ms /    66 runs   (  453.73 ms per run)\n",
      "llama_print_timings:       total time = 45298.61 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Should Tommie react to the observation, and if so, what would be an appropriate reaction? Respond in one line. If the action is to engage in dialogue, write:\n",
      "SAY: \"what to say\"\n",
      "otherwise, write:\n",
      "REACT: Tommie's reaction (if anything).\n",
      "Either do nothing, react, or say something but not both.\n",
      "\n",
      "### Input:\n",
      "Name: Tommie (age: 25)\n",
      "Innate traits: anxious, likes design\n",
      "Tommie is an older man who has fond memories of his childhood pet, Bruno. He is a homebody and enjoys the comforts of home. Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "It is April 23, 2023, 10:28 PM.\n",
      "Tommie's status: looking for a job\n",
      "Summary of relevant context from Tommie's memory:\n",
      "Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "```\n",
      "\n",
      "### Output:\n",
      "The relationship between Tommie and Tommie is that they are both the same person, but in different contexts.\n",
      "```\n",
      "Most recent observations: Tommie remembers his dog, Bruno, from when he was a kid; Tommie feels tired from driving so far; Tommie sees the new home; The new neighbors have a cat; The road is noisy at night; Tommie is hungry; Tommie tries to get some rest.; Tommie observed Beau says What do you like to do? and said \"what to say\".; Tommie observed Tommie wakes up to the sound of a noisy construction site outside his window. and reacted by Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.; Tommie observed Tommie gets out of bed and heads to the kitchen to make himself some coffee. and reacted by Tommie wakes up to the sound of a noisy construction site outside his window.; Tommie observed Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some. and reacted by ```; Tommie observed Tommie finally finds the filters and makes himself a cup of coffee. and reacted by ```; Tommie observed The coffee tastes bitter, and Tommie regrets not buying a better brand. and reacted by ```; Tommie observed Tommie checks his email and sees that he has no job offers yet. and reacted by ```; Tommie observed Tommie spends some time updating his resume and cover letter. and reacted by ```; Tommie observed Tommie heads out to explore the city and look for job openings. and reacted by ```; Tommie observed Tommie sees a sign for a job fair and decides to attend. and reacted by ```; Tommie observed The line to get in is long, and Tommie has to wait for an hour. and reacted by ```; Tommie observed Tommie meets several potential employers at the job fair but doesn't receive any offers. and reacted by ```; Tommie observed Tommie leaves the job fair feeling disappointed. and reacted by ```; Tommie observed Tommie stops by a local diner to grab some lunch. and reacted by ```\n",
      "Observation: The service is slow, and Tommie has to wait for 30 minutes to get his food.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie remembers brushing his teeth\n",
      "\n",
      "### Response:\n",
      "Rating: 1\n",
      "\n",
      "### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie observed The service is slow, and Tommie has to wait for 30 minutes to get his food. and reacted by ```\n",
      "\n",
      "### Response:\n",
      "Rating:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =    11.58 ms /    32 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time = 16472.83 ms /   825 tokens (   19.97 ms per token)\n",
      "llama_print_timings:        eval time = 13824.55 ms /    31 runs   (  445.95 ms per run)\n",
      "llama_print_timings:       total time = 30311.37 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "0.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     1.79 ms /     5 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time =  4051.92 ms /   240 tokens (   16.88 ms per token)\n",
      "llama_print_timings:        eval time =  1555.98 ms /     4 runs   (  389.00 ms per run)\n",
      "llama_print_timings:       total time =  5610.06 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2055.30 ms /    32 tokens (   64.23 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2055.70 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The service is slow, and Tommie has to wait for 30 minutes to get his food. ```\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog is chasing a ball.\n",
      "\n",
      "### Response:\n",
      "The dog\n",
      "\n",
      "### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "Tommie overhears a conversation at the next table about a job opening.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What is The dog doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog jumps into the air and catches the ball.\n",
      "\n",
      "### Response:\n",
      "Catching a ball, possibly playing fetch\n",
      "\n",
      "### Instruction:\n",
      "What is Tommie doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "Tommie overhears a conversation at the next table about a job opening.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     3.54 ms /    10 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time =  2690.23 ms /    91 tokens (   29.56 ms per token)\n",
      "llama_print_timings:        eval time =  3370.51 ms /     9 runs   (  374.50 ms per run)\n",
      "llama_print_timings:       total time =  6064.93 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     4.19 ms /    12 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time =  2633.36 ms /   102 tokens (   25.82 ms per token)\n",
      "llama_print_timings:        eval time =  4126.85 ms /    11 runs   (  375.17 ms per run)\n",
      "llama_print_timings:       total time =  6765.16 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2360.48 ms /    30 tokens (   78.68 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2360.85 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =   844.64 ms /    10 tokens (   84.46 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =   844.80 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Given the provided information, what is the relationship between Tommie and Tommie? Be succinct.\n",
      "\n",
      "### Input:\n",
      "Context from memory:\n",
      "- April 23, 2023, 09:51 PM: Tommie tries to get some rest.\n",
      "- April 23, 2023, 09:50 PM: Tommie remembers his dog, Bruno, from when he was a kid\n",
      "- April 23, 2023, 10:01 PM: Tommie observed Tommie wakes up to the sound of a noisy construction site outside his window. and reacted by Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "- April 23, 2023, 09:51 PM: Tommie is hungry\n",
      "- April 23, 2023, 09:51 PM: The road is noisy at night\n",
      "- April 23, 2023, 09:51 PM: The new neighbors have a cat\n",
      "- April 23, 2023, 10:03 PM: Tommie observed Tommie gets out of bed and heads to the kitchen to make himself some coffee. and reacted by Tommie wakes up to the sound of a noisy construction site outside his window.\n",
      "- April 23, 2023, 10:13 PM: Tommie observed Tommie spends some time updating his resume and cover letter. and reacted by ```\n",
      "- April 23, 2023, 09:50 PM: Tommie feels tired from driving so far\n",
      "- April 23, 2023, 10:16 PM: Tommie observed Tommie heads out to explore the city and look for job openings. and reacted by ```\n",
      "- April 23, 2023, 10:06 PM: Tommie observed Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some. and reacted by ```\n",
      "- April 23, 2023, 10:12 PM: Tommie observed Tommie checks his email and sees that he has no job offers yet. and reacted by ```\n",
      "- April 23, 2023, 10:09 PM: Tommie observed The coffee tastes bitter, and Tommie regrets not buying a better brand. and reacted by ```\n",
      "- April 23, 2023, 10:25 PM: Tommie observed Tommie leaves the job fair feeling disappointed. and reacted by ```\n",
      "- April 23, 2023, 10:17 PM: Tommie observed Tommie sees a sign for a job fair and decides to attend. and reacted by ```\n",
      "- April 23, 2023, 09:50 PM: Tommie sees the new home\n",
      "- April 23, 2023, 10:27 PM: Tommie observed Tommie stops by a local diner to grab some lunch. and reacted by ```\n",
      "- April 23, 2023, 10:29 PM: Tommie observed The service is slow, and Tommie has to wait for 30 minutes to get his food. and reacted by ```\n",
      "- April 23, 2023, 10:08 PM: Tommie observed Tommie finally finds the filters and makes himself a cup of coffee. and reacted by ```\n",
      "- April 23, 2023, 10:20 PM: Tommie observed The line to get in is long, and Tommie has to wait for an hour. and reacted by ```\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =    90.83 ms /   256 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time = 18590.37 ms /   918 tokens (   20.25 ms per token)\n",
      "llama_print_timings:        eval time = 120073.47 ms /   255 runs   (  470.88 ms per run)\n",
      "llama_print_timings:       total time = 138785.64 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Should Tommie react to the observation, and if so, what would be an appropriate reaction? Respond in one line. If the action is to engage in dialogue, write:\n",
      "SAY: \"what to say\"\n",
      "otherwise, write:\n",
      "REACT: Tommie's reaction (if anything).\n",
      "Either do nothing, react, or say something but not both.\n",
      "\n",
      "### Input:\n",
      "Name: Tommie (age: 25)\n",
      "Innate traits: anxious, likes design\n",
      "Tommie is an older man who has fond memories of his childhood pet, Bruno. He is a homebody and enjoys the comforts of home. Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "It is April 23, 2023, 10:31 PM.\n",
      "Tommie's status: looking for a job\n",
      "Summary of relevant context from Tommie's memory:\n",
      "Tommie is a person who is tired from driving so far. He feels the need for rest because the road is noisy at night which makes it difficult to sleep. The new neighbors have a cat, and Tommie wakes up to the sound of a noisy construction site outside his window. Tommie gets out of bed and heads to the kitchen to make himself some coffee. He spends some time updating his resume and cover letter. He then heads out to explore the city and look for job openings. He realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some. Tommie checks his email and sees that he has no job offers yet. The coffee tastes bitter, and Tommie regrets not buying a better brand. Tommie leaves the job fair feeling disappointed. He then sees a sign for a job fair and decides to attend. He stops by a local diner to grab some lunch. The service is slow, and Tommie has to wait for 30 minutes to get his food. Tommie finally finds the filters and makes himself a cup of coffee. He then observes that the line to get in is long,\n",
      "Most recent observations: Tommie remembers his dog, Bruno, from when he was a kid; Tommie feels tired from driving so far; Tommie sees the new home; The new neighbors have a cat; The road is noisy at night; Tommie is hungry; Tommie tries to get some rest.; Tommie observed Beau says What do you like to do? and said \"what to say\".; Tommie observed Tommie wakes up to the sound of a noisy construction site outside his window. and reacted by Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.; Tommie observed Tommie gets out of bed and heads to the kitchen to make himself some coffee. and reacted by Tommie wakes up to the sound of a noisy construction site outside his window.; Tommie observed Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some. and reacted by ```; Tommie observed Tommie finally finds the filters and makes himself a cup of coffee. and reacted by ```; Tommie observed The coffee tastes bitter, and Tommie regrets not buying a better brand. and reacted by ```; Tommie observed Tommie checks his email and sees that he has no job offers yet. and reacted by ```; Tommie observed Tommie spends some time updating his resume and cover letter. and reacted by ```; Tommie observed Tommie heads out to explore the city and look for job openings. and reacted by ```; Tommie observed Tommie sees a sign for a job fair and decides to attend. and reacted by ```; Tommie observed The line to get in is long, and Tommie has to wait for an hour. and reacted by ```; Tommie observed Tommie meets several potential employers at the job fair but doesn't receive any offers. and reacted by ```; Tommie observed Tommie leaves the job fair feeling disappointed. and reacted by ```; Tommie observed Tommie stops by a local diner to grab some lunch. and reacted by ```; Tommie observed The service is slow, and Tommie has to wait for 30 minutes to get his food. and reacted by ```\n",
      "Observation: Tommie overhears a conversation at the next table about a job opening.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie remembers brushing his teeth\n",
      "\n",
      "### Response:\n",
      "Rating: 1\n",
      "\n",
      "### Instruction:\n",
      "On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Respond with a single integer.\n",
      "\n",
      "### Input:\n",
      "Memory: Tommie observed Tommie overhears a conversation at the next table about a job opening. and reacted by ```\n",
      "\n",
      "### Response:\n",
      "Rating:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =    13.47 ms /    37 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time = 26433.43 ms /  1042 tokens (   25.37 ms per token)\n",
      "llama_print_timings:        eval time = 17046.79 ms /    36 runs   (  473.52 ms per run)\n",
      "llama_print_timings:       total time = 43496.95 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "0.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     1.81 ms /     5 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time =  3982.71 ms /   235 tokens (   16.95 ms per token)\n",
      "llama_print_timings:        eval time =  1563.69 ms /     4 runs   (  390.92 ms per run)\n",
      "llama_print_timings:       total time =  5548.61 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2138.43 ms /    27 tokens (   79.20 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2138.83 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tommie overhears a conversation at the next table about a job opening. ```\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog is chasing a ball.\n",
      "\n",
      "### Response:\n",
      "The dog\n",
      "\n",
      "### Instruction:\n",
      "What/who is the subject in the provided observation?\n",
      "\n",
      "### Input:\n",
      "Tommie asks the diners about the job opening and gets some information about the company.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "What is The dog doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "The dog jumps into the air and catches the ball.\n",
      "\n",
      "### Response:\n",
      "Catching a ball, possibly playing fetch\n",
      "\n",
      "### Instruction:\n",
      "What is Tommie doing in the provided observation?\n",
      "\n",
      "### Input:\n",
      "Tommie asks the diners about the job opening and gets some information about the company.\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     3.57 ms /    10 runs   (    0.36 ms per run)\n",
      "llama_print_timings: prompt eval time =  2737.86 ms /    93 tokens (   29.44 ms per token)\n",
      "llama_print_timings:        eval time =  3379.20 ms /     9 runs   (  375.47 ms per run)\n",
      "llama_print_timings:       total time =  6121.36 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3969.01 ms\n",
      "llama_print_timings:      sample time =     8.84 ms /    25 runs   (    0.35 ms per run)\n",
      "llama_print_timings: prompt eval time =  2644.46 ms /   104 tokens (   25.43 ms per token)\n",
      "llama_print_timings:        eval time =  9187.61 ms /    24 runs   (  382.82 ms per run)\n",
      "llama_print_timings:       total time = 11842.71 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  3927.52 ms /    30 tokens (  130.92 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  3927.92 ms\n",
      "\n",
      "llama_print_timings:        load time =  1483.55 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  3890.67 ms /    23 tokens (  169.16 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  3891.31 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m### Instruction:\n",
      "Given the provided information, what is the relationship between Tommie and Tommie? Be succinct.\n",
      "\n",
      "### Input:\n",
      "Context from memory:\n",
      "- April 23, 2023, 09:51 PM: Tommie tries to get some rest.\n",
      "- April 23, 2023, 09:50 PM: Tommie remembers his dog, Bruno, from when he was a kid\n",
      "- April 23, 2023, 10:01 PM: Tommie observed Tommie wakes up to the sound of a noisy construction site outside his window. and reacted by Tommie is tired from driving long distances and feels the need for rest. The road is noisy at night which makes it difficult to sleep.\n",
      "- April 23, 2023, 09:51 PM: Tommie is hungry\n",
      "- April 23, 2023, 09:51 PM: The road is noisy at night\n",
      "- April 23, 2023, 09:51 PM: The new neighbors have a cat\n",
      "- April 23, 2023, 10:03 PM: Tommie observed Tommie gets out of bed and heads to the kitchen to make himself some coffee. and reacted by Tommie wakes up to the sound of a noisy construction site outside his window.\n",
      "- April 23, 2023, 10:13 PM: Tommie observed Tommie spends some time updating his resume and cover letter. and reacted by ```\n",
      "- April 23, 2023, 09:50 PM: Tommie feels tired from driving so far\n",
      "- April 23, 2023, 10:32 PM: Tommie observed Tommie overhears a conversation at the next table about a job opening. and reacted by ```\n",
      "- April 23, 2023, 10:16 PM: Tommie observed Tommie heads out to explore the city and look for job openings. and reacted by ```\n",
      "- April 23, 2023, 10:06 PM: Tommie observed Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some. and reacted by ```\n",
      "- April 23, 2023, 10:12 PM: Tommie observed Tommie checks his email and sees that he has no job offers yet. and reacted by ```\n",
      "- April 23, 2023, 10:09 PM: Tommie observed The coffee tastes bitter, and Tommie regrets not buying a better brand. and reacted by ```\n",
      "- April 23, 2023, 10:25 PM: Tommie observed Tommie leaves the job fair feeling disappointed. and reacted by ```\n",
      "- April 23, 2023, 09:50 PM: Tommie sees the new home\n",
      "- April 23, 2023, 10:27 PM: Tommie observed Tommie stops by a local diner to grab some lunch. and reacted by ```\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Let's send Tommie on their way. We'll check in on their summary every few observations to watch it evolve\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, observation \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(observations):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     _, reaction \u001b[39m=\u001b[39m tommie\u001b[39m.\u001b[39;49mgenerate_reaction(observation)\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(colored(observation, \u001b[39m\"\u001b[39m\u001b[39mgreen\u001b[39m\u001b[39m\"\u001b[39m), reaction)\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mif\u001b[39;00m ((i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m20\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32m/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb Cell 15\u001b[0m in \u001b[0;36mGenerativeAgent.generate_reaction\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=241'>242</a>\u001b[0m \u001b[39m\"\"\"React to a given observation.\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=242'>243</a>\u001b[0m call_to_action_template \u001b[39m=\u001b[39m (\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=243'>244</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mShould \u001b[39m\u001b[39m{agent_name}\u001b[39;00m\u001b[39m react to the observation, and if so,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=244'>245</a>\u001b[0m     \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m what would be an appropriate reaction? Respond in one line.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=247'>248</a>\u001b[0m     \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mEither do nothing, react, or say something but not both.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=248'>249</a>\u001b[0m )\n\u001b[0;32m--> <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=249'>250</a>\u001b[0m full_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_reaction(observation, call_to_action_template)\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=250'>251</a>\u001b[0m result \u001b[39m=\u001b[39m full_result\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=251'>252</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_memory(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m observed \u001b[39m\u001b[39m{\u001b[39;00mobservation\u001b[39m}\u001b[39;00m\u001b[39m and reacted by \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb Cell 15\u001b[0m in \u001b[0;36mGenerativeAgent._generate_reaction\u001b[0;34m(self, observation, task)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=212'>213</a>\u001b[0m prompt \u001b[39m=\u001b[39m PromptTemplate\u001b[39m.\u001b[39mfrom_template(\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=213'>214</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m### Instruction:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=214'>215</a>\u001b[0m         \u001b[39m+\u001b[39m task \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=223'>224</a>\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m### Response:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=224'>225</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=225'>226</a>\u001b[0m agent_summary_description \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_summary()\n\u001b[0;32m--> <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=226'>227</a>\u001b[0m relevant_memories_str \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msummarize_related_memories(observation)\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=227'>228</a>\u001b[0m current_time_str \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mB \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%\u001b[39m\u001b[39mY, \u001b[39m\u001b[39m%\u001b[39m\u001b[39mI:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM \u001b[39m\u001b[39m%\u001b[39m\u001b[39mp\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=228'>229</a>\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(agent_summary_description\u001b[39m=\u001b[39magent_summary_description,\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=229'>230</a>\u001b[0m               current_time\u001b[39m=\u001b[39mcurrent_time_str,\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=230'>231</a>\u001b[0m               relevant_memories\u001b[39m=\u001b[39mrelevant_memories_str,\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=231'>232</a>\u001b[0m               agent_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname,\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=232'>233</a>\u001b[0m               observation\u001b[39m=\u001b[39mobservation,\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=233'>234</a>\u001b[0m              agent_status\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus)\n",
      "\u001b[1;32m/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb Cell 15\u001b[0m in \u001b[0;36mGenerativeAgent.summarize_related_memories\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=189'>190</a>\u001b[0m prompt \u001b[39m=\u001b[39m PromptTemplate\u001b[39m.\u001b[39mfrom_template(\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=190'>191</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m{q1}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m### Input:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mContext from memory:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{context_str}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m### Response:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=191'>192</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=192'>193</a>\u001b[0m chain \u001b[39m=\u001b[39m LLMChain(llm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm, prompt\u001b[39m=\u001b[39mprompt, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/media/captdishwasher/Samshmung/horenbergerb/llama/Local-LLM-Langchain/generative_agent.ipynb#X21sZmlsZQ%3D%3D?line=193'>194</a>\u001b[0m \u001b[39mreturn\u001b[39;00m chain\u001b[39m.\u001b[39;49mrun(q1\u001b[39m=\u001b[39;49mq1, context_str\u001b[39m=\u001b[39;49mcontext_str\u001b[39m.\u001b[39;49mstrip())\u001b[39m.\u001b[39mstrip()\n",
      "File \u001b[0;32m/media/captdishwasher/Samshmung/horenbergerb/llama/langchain/langchain/chains/base.py:216\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(args[\u001b[39m0\u001b[39m])[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    215\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m--> 216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    218\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    219\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`run` supported with either positional arguments or keyword arguments\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    220\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m but not both. Got args: \u001b[39m\u001b[39m{\u001b[39;00margs\u001b[39m}\u001b[39;00m\u001b[39m and kwargs: \u001b[39m\u001b[39m{\u001b[39;00mkwargs\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    221\u001b[0m )\n",
      "File \u001b[0;32m/media/captdishwasher/Samshmung/horenbergerb/llama/langchain/langchain/chains/base.py:116\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_end(outputs, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m    118\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[0;32m/media/captdishwasher/Samshmung/horenbergerb/llama/langchain/langchain/chains/base.py:113\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    108\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m    109\u001b[0m     inputs,\n\u001b[1;32m    110\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m    111\u001b[0m )\n\u001b[1;32m    112\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs)\n\u001b[1;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[0;32m/media/captdishwasher/Samshmung/horenbergerb/llama/langchain/langchain/chains/llm.py:57\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\u001b[39mself\u001b[39m, inputs: Dict[\u001b[39mstr\u001b[39m, Any]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply([inputs])[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/media/captdishwasher/Samshmung/horenbergerb/llama/langchain/langchain/chains/llm.py:118\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[0;34m(self, input_list)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mself\u001b[39m, input_list: List[Dict[\u001b[39mstr\u001b[39m, Any]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]]:\n\u001b[1;32m    117\u001b[0m     \u001b[39m\"\"\"Utilize the LLM generate method for speed gains.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(input_list)\n\u001b[1;32m    119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)\n",
      "File \u001b[0;32m/media/captdishwasher/Samshmung/horenbergerb/llama/langchain/langchain/chains/llm.py:62\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list)\n\u001b[0;32m---> 62\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate_prompt(prompts, stop)\n",
      "File \u001b[0;32m/media/captdishwasher/Samshmung/horenbergerb/llama/langchain/langchain/llms/base.py:107\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    104\u001b[0m     \u001b[39mself\u001b[39m, prompts: List[PromptValue], stop: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    105\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    106\u001b[0m     prompt_strings \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_string() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 107\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_strings, stop\u001b[39m=\u001b[39;49mstop)\n",
      "File \u001b[0;32m/media/captdishwasher/Samshmung/horenbergerb/llama/langchain/langchain/llms/base.py:140\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    141\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_end(output, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m    142\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/media/captdishwasher/Samshmung/horenbergerb/llama/langchain/langchain/llms/base.py:137\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    134\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m}, prompts, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose\n\u001b[1;32m    135\u001b[0m )\n\u001b[1;32m    136\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(prompts, stop\u001b[39m=\u001b[39;49mstop)\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[0;32m/media/captdishwasher/Samshmung/horenbergerb/llama/langchain/langchain/llms/base.py:324\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    322\u001b[0m generations \u001b[39m=\u001b[39m []\n\u001b[1;32m    323\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[0;32m--> 324\u001b[0m     text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(prompt, stop\u001b[39m=\u001b[39;49mstop)\n\u001b[1;32m    325\u001b[0m     generations\u001b[39m.\u001b[39mappend([Generation(text\u001b[39m=\u001b[39mtext)])\n\u001b[1;32m    326\u001b[0m \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m/media/captdishwasher/Samshmung/horenbergerb/llama/langchain/langchain/llms/llamacpp.py:201\u001b[0m, in \u001b[0;36mLlamaCpp._call\u001b[0;34m(self, prompt, stop)\u001b[0m\n\u001b[1;32m    198\u001b[0m     params[\u001b[39m\"\u001b[39m\u001b[39mstop_sequences\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m []\n\u001b[1;32m    200\u001b[0m \u001b[39m\"\"\"Call the Llama model and return the output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient(\n\u001b[1;32m    202\u001b[0m     prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[1;32m    203\u001b[0m     max_tokens\u001b[39m=\u001b[39;49mparams[\u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    204\u001b[0m     temperature\u001b[39m=\u001b[39;49mparams[\u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    205\u001b[0m     top_p\u001b[39m=\u001b[39;49mparams[\u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    206\u001b[0m     logprobs\u001b[39m=\u001b[39;49mparams[\u001b[39m\"\u001b[39;49m\u001b[39mlogprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    207\u001b[0m     echo\u001b[39m=\u001b[39;49mparams[\u001b[39m\"\u001b[39;49m\u001b[39mecho\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    208\u001b[0m     stop\u001b[39m=\u001b[39;49mparams[\u001b[39m\"\u001b[39;49m\u001b[39mstop_sequences\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    209\u001b[0m     repeat_penalty\u001b[39m=\u001b[39;49mparams[\u001b[39m\"\u001b[39;49m\u001b[39mrepeat_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    210\u001b[0m     top_k\u001b[39m=\u001b[39;49mparams[\u001b[39m\"\u001b[39;49m\u001b[39mtop_k\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    211\u001b[0m )\n\u001b[1;32m    212\u001b[0m \u001b[39mreturn\u001b[39;00m text[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/media/captdishwasher/Samshmung/horenbergerb/llama/llama-cpp-python/llama_cpp/llama.py:681\u001b[0m, in \u001b[0;36mLlama.__call__\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, repeat_penalty, top_k, stream)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m    646\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    647\u001b[0m     prompt: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    657\u001b[0m     stream: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    658\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Completion, Iterator[CompletionChunk]]:\n\u001b[1;32m    659\u001b[0m     \u001b[39m\"\"\"Generate text from a prompt.\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \n\u001b[1;32m    661\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[39m        Response object containing the generated text.\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_completion(\n\u001b[1;32m    682\u001b[0m         prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[1;32m    683\u001b[0m         suffix\u001b[39m=\u001b[39;49msuffix,\n\u001b[1;32m    684\u001b[0m         max_tokens\u001b[39m=\u001b[39;49mmax_tokens,\n\u001b[1;32m    685\u001b[0m         temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m    686\u001b[0m         top_p\u001b[39m=\u001b[39;49mtop_p,\n\u001b[1;32m    687\u001b[0m         logprobs\u001b[39m=\u001b[39;49mlogprobs,\n\u001b[1;32m    688\u001b[0m         echo\u001b[39m=\u001b[39;49mecho,\n\u001b[1;32m    689\u001b[0m         stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    690\u001b[0m         repeat_penalty\u001b[39m=\u001b[39;49mrepeat_penalty,\n\u001b[1;32m    691\u001b[0m         top_k\u001b[39m=\u001b[39;49mtop_k,\n\u001b[1;32m    692\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    693\u001b[0m     )\n",
      "File \u001b[0;32m/media/captdishwasher/Samshmung/horenbergerb/llama/llama-cpp-python/llama_cpp/llama.py:642\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, repeat_penalty, top_k, stream)\u001b[0m\n\u001b[1;32m    640\u001b[0m     chunks: Iterator[CompletionChunk] \u001b[39m=\u001b[39m completion_or_chunks\n\u001b[1;32m    641\u001b[0m     \u001b[39mreturn\u001b[39;00m chunks\n\u001b[0;32m--> 642\u001b[0m completion: Completion \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(completion_or_chunks)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \u001b[39mreturn\u001b[39;00m completion\n",
      "File \u001b[0;32m/media/captdishwasher/Samshmung/horenbergerb/llama/llama-cpp-python/llama_cpp/llama.py:436\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, repeat_penalty, top_k, stream)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[39m###\u001b[39;00m\n\u001b[1;32m    435\u001b[0m finish_reason \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlength\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 436\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m    437\u001b[0m     prompt_tokens,\n\u001b[1;32m    438\u001b[0m     top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m    439\u001b[0m     top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[1;32m    440\u001b[0m     temp\u001b[39m=\u001b[39mtemperature,\n\u001b[1;32m    441\u001b[0m     repeat_penalty\u001b[39m=\u001b[39mrepeat_penalty,\n\u001b[1;32m    442\u001b[0m     reset\u001b[39m=\u001b[39mreset,\n\u001b[1;32m    443\u001b[0m ):\n\u001b[1;32m    444\u001b[0m     \u001b[39mif\u001b[39;00m token \u001b[39m==\u001b[39m llama_cpp\u001b[39m.\u001b[39mllama_token_eos():\n\u001b[1;32m    445\u001b[0m         text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdetokenize(completion_tokens)\n",
      "File \u001b[0;32m/media/captdishwasher/Samshmung/horenbergerb/llama/llama-cpp-python/llama_cpp/llama.py:307\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, temp, repeat_penalty, reset)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset()\n\u001b[1;32m    306\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 307\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval(tokens)\n\u001b[1;32m    308\u001b[0m     token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample(\n\u001b[1;32m    309\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m    310\u001b[0m         top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[1;32m    311\u001b[0m         temp\u001b[39m=\u001b[39mtemp,\n\u001b[1;32m    312\u001b[0m         repeat_penalty\u001b[39m=\u001b[39mrepeat_penalty,\n\u001b[1;32m    313\u001b[0m     )\n\u001b[1;32m    314\u001b[0m     tokens_or_none \u001b[39m=\u001b[39m \u001b[39myield\u001b[39;00m token\n",
      "File \u001b[0;32m/media/captdishwasher/Samshmung/horenbergerb/llama/llama-cpp-python/llama_cpp/llama.py:205\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_past \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(n_ctx \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(batch), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokens_consumed)\n\u001b[1;32m    204\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_tokens \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(batch)\n\u001b[0;32m--> 205\u001b[0m return_code \u001b[39m=\u001b[39m llama_cpp\u001b[39m.\u001b[39;49mllama_eval(\n\u001b[1;32m    206\u001b[0m     ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx,\n\u001b[1;32m    207\u001b[0m     tokens\u001b[39m=\u001b[39;49m(llama_cpp\u001b[39m.\u001b[39;49mllama_token \u001b[39m*\u001b[39;49m \u001b[39mlen\u001b[39;49m(batch))(\u001b[39m*\u001b[39;49mbatch),\n\u001b[1;32m    208\u001b[0m     n_tokens\u001b[39m=\u001b[39;49mllama_cpp\u001b[39m.\u001b[39;49mc_int(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_tokens),\n\u001b[1;32m    209\u001b[0m     n_past\u001b[39m=\u001b[39;49mllama_cpp\u001b[39m.\u001b[39;49mc_int(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_past),\n\u001b[1;32m    210\u001b[0m     n_threads\u001b[39m=\u001b[39;49mllama_cpp\u001b[39m.\u001b[39;49mc_int(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_threads),\n\u001b[1;32m    211\u001b[0m )\n\u001b[1;32m    212\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mint\u001b[39m(return_code) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    213\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mllama_eval returned \u001b[39m\u001b[39m{\u001b[39;00mreturn_code\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/media/captdishwasher/Samshmung/horenbergerb/llama/llama-cpp-python/llama_cpp/llama_cpp.py:284\u001b[0m, in \u001b[0;36mllama_eval\u001b[0;34m(ctx, tokens, n_tokens, n_past, n_threads)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mllama_eval\u001b[39m(\n\u001b[1;32m    278\u001b[0m     ctx: llama_context_p,\n\u001b[1;32m    279\u001b[0m     tokens,  \u001b[39m# type: Array[llama_token]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    282\u001b[0m     n_threads: c_int,\n\u001b[1;32m    283\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m c_int:\n\u001b[0;32m--> 284\u001b[0m     \u001b[39mreturn\u001b[39;00m _lib\u001b[39m.\u001b[39;49mllama_eval(ctx, tokens, n_tokens, n_past, n_threads)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Let's send Tommie on their way. We'll check in on their summary every few observations to watch it evolve\n",
    "for i, observation in enumerate(observations):\n",
    "    _, reaction = tommie.generate_reaction(observation)\n",
    "    print(colored(observation, \"green\"), reaction)\n",
    "    if ((i+1) % 20) == 0:\n",
    "        print('*'*40)\n",
    "        print(colored(f\"After {i+1} observations, Tommie's summary is:\\n{tommie.get_summary(force_refresh=True)}\", \"blue\"))\n",
    "        print('*'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interview_agent(tommie, \"Tell me about how your day has been going\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interview_agent(tommie, \"How do you feel about coffee?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interview_agent(tommie, \"Tell me about your childhood dog!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
